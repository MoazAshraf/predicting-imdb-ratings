{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cgCpIyJuT1Zb"
   },
   "source": [
    "## Shortlist Promising Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes the data has been split into a training and a test set. If not, run get_data.ipynb first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HTOZvrk8Ye-X"
   },
   "source": [
    "1. Try these models:\n",
    "  - Linear Regression\n",
    "  - Random Forest Regressor\n",
    "  - Dense Neural Network\n",
    "  - Linear SVR\n",
    "2. Measure and compare their performance on RMSE (compare means and standard deviations of RMSE for different models as well)\n",
    "3. Make a quick round of feature selection and engineering:\n",
    "  - Try transforming variables to normal distributions\n",
    "  - Try removing unimportant features\n",
    "  - Try adding polynomial features\n",
    "4. Perform one or two more quick iterations of the five previous steps.\n",
    "5. Shortlist the top three to five most promising models, preferring models that\n",
    "make different types of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TRAINING_FILEPATH = 'data/training_set.csv'\n",
    "TEST_FILEPATH = 'data/test_set.csv'\n",
    "\n",
    "training_set = pd.read_csv(TRAINING_FILEPATH, index_col='index')\n",
    "test_set = pd.read_csv(TEST_FILEPATH, index_col='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_utils import separate_features_targets, FeaturePreprocessor\n",
    "\n",
    "train_X, train_y = separate_features_targets(training_set)\n",
    "\n",
    "# preprocess training features: power transform\n",
    "feature_preprocessor = FeaturePreprocessor(add_combinations=True, powertransform_num=True, onehot_type=True)\n",
    "train_X = feature_preprocessor.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rTq8z67lWNxq",
    "outputId": "a9ab2413-b5f3-4273-bd42-69f3ddbf27f3"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1.1203423450376466"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# the baseline RMSE is the standard deviation of the targets\n",
    "train_y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vv8ClUkjf0tN"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                  fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\nbaseline train  1.123358  1.121115  1.114294  1.119928  1.122722  1.120283   \nbaseline val    1.107938  1.116999  1.143970  1.121741  1.110553  1.120240   \n\n                     std  \nbaseline train  0.003609  \nbaseline val    0.014327  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fold 0</th>\n      <th>fold 1</th>\n      <th>fold 2</th>\n      <th>fold 3</th>\n      <th>fold 4</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>baseline train</th>\n      <td>1.123358</td>\n      <td>1.121115</td>\n      <td>1.114294</td>\n      <td>1.119928</td>\n      <td>1.122722</td>\n      <td>1.120283</td>\n      <td>0.003609</td>\n    </tr>\n    <tr>\n      <th>baseline val</th>\n      <td>1.107938</td>\n      <td>1.116999</td>\n      <td>1.143970</td>\n      <td>1.121741</td>\n      <td>1.110553</td>\n      <td>1.120240</td>\n      <td>0.014327</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from train_utils import cross_val_rmse\n",
    "\n",
    "baseline_model = DummyRegressor(strategy='mean')\n",
    "baseline_errors = cross_val_rmse(baseline_model, train_X, train_y, cv=5, random_state=42, model_name='baseline')\n",
    "display(baseline_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ru-7KyWcjZuf"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\nlinreg train  0.893267  0.896926  0.889980  0.890549  0.894051  0.892955   \nlinreg val    0.897161  0.881527  0.910644  0.908679  0.894478  0.898498   \n\n                   std  \nlinreg train  0.002815  \nlinreg val    0.011802  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fold 0</th>\n      <th>fold 1</th>\n      <th>fold 2</th>\n      <th>fold 3</th>\n      <th>fold 4</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>linreg train</th>\n      <td>0.893267</td>\n      <td>0.896926</td>\n      <td>0.889980</td>\n      <td>0.890549</td>\n      <td>0.894051</td>\n      <td>0.892955</td>\n      <td>0.002815</td>\n    </tr>\n    <tr>\n      <th>linreg val</th>\n      <td>0.897161</td>\n      <td>0.881527</td>\n      <td>0.910644</td>\n      <td>0.908679</td>\n      <td>0.894478</td>\n      <td>0.898498</td>\n      <td>0.011802</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg_model = LinearRegression()\n",
    "linreg_errors = cross_val_rmse(linreg_model, train_X, train_y, cv=5, random_state=42, model_name='linreg')\n",
    "display(linreg_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SUv59CtFnpHo"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                   fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\nforestreg train  0.348015  0.354293  0.351649  0.364987  0.350212  0.353831   \nforestreg val    0.858008  0.847944  0.852273  0.844156  0.828591  0.846194   \n\n                      std  \nforestreg train  0.006640  \nforestreg val    0.011109  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fold 0</th>\n      <th>fold 1</th>\n      <th>fold 2</th>\n      <th>fold 3</th>\n      <th>fold 4</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>forestreg train</th>\n      <td>0.348015</td>\n      <td>0.354293</td>\n      <td>0.351649</td>\n      <td>0.364987</td>\n      <td>0.350212</td>\n      <td>0.353831</td>\n      <td>0.006640</td>\n    </tr>\n    <tr>\n      <th>forestreg val</th>\n      <td>0.858008</td>\n      <td>0.847944</td>\n      <td>0.852273</td>\n      <td>0.844156</td>\n      <td>0.828591</td>\n      <td>0.846194</td>\n      <td>0.011109</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forestreg_model = RandomForestRegressor()\n",
    "forestreg_errors = cross_val_rmse(forestreg_model, train_X, train_y, cv=5, random_state=42, model_name='forestreg')\n",
    "display(forestreg_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYF17CVpjnTm"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\nlinsvr train  0.906312  0.911642  0.903500  0.902954  0.907608  0.906403   \nlinsvr val    0.908033  0.893220  0.923385  0.920661  0.903200  0.909700   \n\n                   std  \nlinsvr train  0.003508  \nlinsvr val    0.012490  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fold 0</th>\n      <th>fold 1</th>\n      <th>fold 2</th>\n      <th>fold 3</th>\n      <th>fold 4</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>linsvr train</th>\n      <td>0.906312</td>\n      <td>0.911642</td>\n      <td>0.903500</td>\n      <td>0.902954</td>\n      <td>0.907608</td>\n      <td>0.906403</td>\n      <td>0.003508</td>\n    </tr>\n    <tr>\n      <th>linsvr val</th>\n      <td>0.908033</td>\n      <td>0.893220</td>\n      <td>0.923385</td>\n      <td>0.920661</td>\n      <td>0.903200</td>\n      <td>0.909700</td>\n      <td>0.012490</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "linsvr_model = LinearSVR(max_iter=100000)\n",
    "linsvr_errors = cross_val_rmse(linsvr_model, train_X, train_y, cv=5, random_state=42, model_name='linsvr')\n",
    "display(linsvr_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0m_pQSHJj23c",
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=========] - 0s 18us/step - loss: 0.5967\nEpoch 70/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5946\nEpoch 71/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5968\nEpoch 72/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5949\nEpoch 73/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5951\nEpoch 74/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5928\nEpoch 75/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5926\nEpoch 76/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5907\nEpoch 77/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5884\nEpoch 78/150\n8672/8672 [==============================] - 0s 21us/step - loss: 0.5914\nEpoch 79/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5874\nEpoch 80/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5851\nEpoch 81/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.5856\nEpoch 82/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5843\nEpoch 83/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.5845\nEpoch 84/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5850\nEpoch 85/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5850\nEpoch 86/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5793\nEpoch 87/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5808\nEpoch 88/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5808\nEpoch 89/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5765\nEpoch 90/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5801\nEpoch 91/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5796\nEpoch 92/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5788\nEpoch 93/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5778\nEpoch 94/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5796\nEpoch 95/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.5762\nEpoch 96/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.5752\nEpoch 97/150\n8672/8672 [==============================] - 0s 23us/step - loss: 0.5735\nEpoch 98/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5718\nEpoch 99/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5757\nEpoch 100/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5746\nEpoch 101/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.5731\nEpoch 102/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5715\nEpoch 103/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5725\nEpoch 104/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5721\nEpoch 105/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5699\nEpoch 106/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5705\nEpoch 107/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5684\nEpoch 108/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5668\nEpoch 109/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5660\nEpoch 110/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5660\nEpoch 111/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5679\nEpoch 112/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5670\nEpoch 113/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5662\nEpoch 114/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5647\nEpoch 115/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.5668\nEpoch 116/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5642\nEpoch 117/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5634\nEpoch 118/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5640\nEpoch 119/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5628\nEpoch 120/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.5620\nEpoch 121/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5637\nEpoch 122/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5638\nEpoch 123/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5615\nEpoch 124/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5615\nEpoch 125/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5617\nEpoch 126/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5613\nEpoch 127/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5611\nEpoch 128/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5610\nEpoch 129/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5588\nEpoch 130/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5583\nEpoch 131/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5585\nEpoch 132/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5589\nEpoch 133/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.5569\nEpoch 134/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5583\nEpoch 135/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5556\nEpoch 136/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5553\nEpoch 137/150\n8672/8672 [==============================] - 0s 21us/step - loss: 0.5555\nEpoch 138/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5564\nEpoch 139/150\n8672/8672 [==============================] - 0s 23us/step - loss: 0.5551\nEpoch 140/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5554\nEpoch 141/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5561\nEpoch 142/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5523\nEpoch 143/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5531\nEpoch 144/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5521\nEpoch 145/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.5560\nEpoch 146/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5530\nEpoch 147/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5499\nEpoch 148/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5498\nEpoch 149/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5484\nEpoch 150/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5519\nEpoch 1/150\n8672/8672 [==============================] - 0s 31us/step - loss: 2.0295\nEpoch 2/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.8645\nEpoch 3/150\n8672/8672 [==============================] - 0s 21us/step - loss: 0.8005\nEpoch 4/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.7709\nEpoch 5/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.7543\nEpoch 6/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.7385\nEpoch 7/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.7248\nEpoch 8/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.7144\nEpoch 9/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.7050\nEpoch 10/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6990\nEpoch 11/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.6916\nEpoch 12/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.6856\nEpoch 13/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6817\nEpoch 14/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6738\nEpoch 15/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6711\nEpoch 16/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.6662\nEpoch 17/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6656\nEpoch 18/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6618\nEpoch 19/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6564\nEpoch 20/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6515\nEpoch 21/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.6500\nEpoch 22/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6455\nEpoch 23/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6425\nEpoch 24/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.6388\nEpoch 25/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6386\nEpoch 26/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6338\nEpoch 27/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6343\nEpoch 28/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6313\nEpoch 29/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6308\nEpoch 30/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6268\nEpoch 31/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6267\nEpoch 32/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6234\nEpoch 33/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6197\nEpoch 34/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.6217\nEpoch 35/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6166\nEpoch 36/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6178\nEpoch 37/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6131\nEpoch 38/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6144\nEpoch 39/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6139\nEpoch 40/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.6126\nEpoch 41/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6087\nEpoch 42/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.6081\nEpoch 43/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6080\nEpoch 44/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6037\nEpoch 45/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6030\nEpoch 46/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6021\nEpoch 47/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6021\nEpoch 48/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5979\nEpoch 49/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.6032\nEpoch 50/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5986\nEpoch 51/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5946\nEpoch 52/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5969\nEpoch 53/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5950\nEpoch 54/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5930\nEpoch 55/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5913\nEpoch 56/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5937\nEpoch 57/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5916\nEpoch 58/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5913\nEpoch 59/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5919\nEpoch 60/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5904\nEpoch 61/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5863\nEpoch 62/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5885\nEpoch 63/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5883\nEpoch 64/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5840\nEpoch 65/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5836\nEpoch 66/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5849\nEpoch 67/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5804\nEpoch 68/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5804\nEpoch 69/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5830\nEpoch 70/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5823\nEpoch 71/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5848\nEpoch 72/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5784\nEpoch 73/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5780\nEpoch 74/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5777\nEpoch 75/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5783\nEpoch 76/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5776\nEpoch 77/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5781\nEpoch 78/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5764\nEpoch 79/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5754\nEpoch 80/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5743\nEpoch 81/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5768\nEpoch 82/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5703\nEpoch 83/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5726\nEpoch 84/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5730\nEpoch 85/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5736\nEpoch 86/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5725\nEpoch 87/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5711\nEpoch 88/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5702\nEpoch 89/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5688\nEpoch 90/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5707\nEpoch 91/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5712\nEpoch 92/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5684\nEpoch 93/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5666\nEpoch 94/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5656\nEpoch 95/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5677\nEpoch 96/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5669\nEpoch 97/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5696\nEpoch 98/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5649\nEpoch 99/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5673\nEpoch 100/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5670\nEpoch 101/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5644\nEpoch 102/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5629\nEpoch 103/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5653\nEpoch 104/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5621\nEpoch 105/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5636\nEpoch 106/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5631\nEpoch 107/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5607\nEpoch 108/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5613\nEpoch 109/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5616\nEpoch 110/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5604\nEpoch 111/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5611\nEpoch 112/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5587\nEpoch 113/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5608\nEpoch 114/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5566\nEpoch 115/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5592\nEpoch 116/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5554\nEpoch 117/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.5595\nEpoch 118/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5579\nEpoch 119/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5584\nEpoch 120/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5571\nEpoch 121/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5566\nEpoch 122/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5571\nEpoch 123/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5527\nEpoch 124/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5561\nEpoch 125/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5556\nEpoch 126/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5517\nEpoch 127/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5542\nEpoch 128/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5540\nEpoch 129/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5519\nEpoch 130/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5534\nEpoch 131/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5535\nEpoch 132/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5518\nEpoch 133/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5506\nEpoch 134/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5547\nEpoch 135/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5526\nEpoch 136/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5509\nEpoch 137/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5483\nEpoch 138/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5507\nEpoch 139/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5503\nEpoch 140/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5516\nEpoch 141/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5507\nEpoch 142/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5496\nEpoch 143/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5524\nEpoch 144/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5481\nEpoch 145/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5493\nEpoch 146/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5477\nEpoch 147/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5511\nEpoch 148/150\n8672/8672 [==============================] - 0s 14us/step - loss: 0.5453\nEpoch 149/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5471\nEpoch 150/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5497\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                   fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\nneuralnet train  0.726723  0.730062  0.737216  0.727451  0.728118  0.729914   \nneuralnet val    0.819689  0.811171  0.827034  0.812550  0.810170  0.816123   \n\n                      std  \nneuralnet train  0.004266  \nneuralnet val    0.007150  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fold 0</th>\n      <th>fold 1</th>\n      <th>fold 2</th>\n      <th>fold 3</th>\n      <th>fold 4</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>neuralnet train</th>\n      <td>0.726723</td>\n      <td>0.730062</td>\n      <td>0.737216</td>\n      <td>0.727451</td>\n      <td>0.728118</td>\n      <td>0.729914</td>\n      <td>0.004266</td>\n    </tr>\n    <tr>\n      <th>neuralnet val</th>\n      <td>0.819689</td>\n      <td>0.811171</td>\n      <td>0.827034</td>\n      <td>0.812550</td>\n      <td>0.810170</td>\n      <td>0.816123</td>\n      <td>0.007150</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "def create_neuralnet_model(input_shape):\n",
    "    def create_model():\n",
    "        model = Sequential([\n",
    "                            Dense(32, input_shape=input_shape, activation='relu'),\n",
    "                            Dense(1)\n",
    "                ])\n",
    "        model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "        return model\n",
    "\n",
    "    # wrap the neural network model to be used by scikit-learn\n",
    "    neuralnet_model = KerasRegressor(create_model, epochs=150)\n",
    "    return neuralnet_model\n",
    "\n",
    "neuralnet_model = create_neuralnet_model(train_X.shape[1:])\n",
    "neuralnet_errors = cross_val_rmse(neuralnet_model, train_X, train_y, cv=5, random_state=42, model_name='neuralnet')\n",
    "display(neuralnet_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SkPo0DwQu_f9"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                   fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\nbaseline train   1.123358  1.121115  1.114294  1.119928  1.122722  1.120283   \nbaseline val     1.107938  1.116999  1.143970  1.121741  1.110553  1.120240   \nlinreg train     0.893267  0.896926  0.889980  0.890549  0.894051  0.892955   \nlinreg val       0.897161  0.881527  0.910644  0.908679  0.894478  0.898498   \nforestreg train  0.348015  0.354293  0.351649  0.364987  0.350212  0.353831   \nforestreg val    0.858008  0.847944  0.852273  0.844156  0.828591  0.846194   \nlinsvr train     0.906312  0.911642  0.903500  0.902954  0.907608  0.906403   \nlinsvr val       0.908033  0.893220  0.923385  0.920661  0.903200  0.909700   \nneuralnet train  0.726723  0.730062  0.737216  0.727451  0.728118  0.729914   \nneuralnet val    0.819689  0.811171  0.827034  0.812550  0.810170  0.816123   \n\n                      std  \nbaseline train   0.003609  \nbaseline val     0.014327  \nlinreg train     0.002815  \nlinreg val       0.011802  \nforestreg train  0.006640  \nforestreg val    0.011109  \nlinsvr train     0.003508  \nlinsvr val       0.012490  \nneuralnet train  0.004266  \nneuralnet val    0.007150  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fold 0</th>\n      <th>fold 1</th>\n      <th>fold 2</th>\n      <th>fold 3</th>\n      <th>fold 4</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>baseline train</th>\n      <td>1.123358</td>\n      <td>1.121115</td>\n      <td>1.114294</td>\n      <td>1.119928</td>\n      <td>1.122722</td>\n      <td>1.120283</td>\n      <td>0.003609</td>\n    </tr>\n    <tr>\n      <th>baseline val</th>\n      <td>1.107938</td>\n      <td>1.116999</td>\n      <td>1.143970</td>\n      <td>1.121741</td>\n      <td>1.110553</td>\n      <td>1.120240</td>\n      <td>0.014327</td>\n    </tr>\n    <tr>\n      <th>linreg train</th>\n      <td>0.893267</td>\n      <td>0.896926</td>\n      <td>0.889980</td>\n      <td>0.890549</td>\n      <td>0.894051</td>\n      <td>0.892955</td>\n      <td>0.002815</td>\n    </tr>\n    <tr>\n      <th>linreg val</th>\n      <td>0.897161</td>\n      <td>0.881527</td>\n      <td>0.910644</td>\n      <td>0.908679</td>\n      <td>0.894478</td>\n      <td>0.898498</td>\n      <td>0.011802</td>\n    </tr>\n    <tr>\n      <th>forestreg train</th>\n      <td>0.348015</td>\n      <td>0.354293</td>\n      <td>0.351649</td>\n      <td>0.364987</td>\n      <td>0.350212</td>\n      <td>0.353831</td>\n      <td>0.006640</td>\n    </tr>\n    <tr>\n      <th>forestreg val</th>\n      <td>0.858008</td>\n      <td>0.847944</td>\n      <td>0.852273</td>\n      <td>0.844156</td>\n      <td>0.828591</td>\n      <td>0.846194</td>\n      <td>0.011109</td>\n    </tr>\n    <tr>\n      <th>linsvr train</th>\n      <td>0.906312</td>\n      <td>0.911642</td>\n      <td>0.903500</td>\n      <td>0.902954</td>\n      <td>0.907608</td>\n      <td>0.906403</td>\n      <td>0.003508</td>\n    </tr>\n    <tr>\n      <th>linsvr val</th>\n      <td>0.908033</td>\n      <td>0.893220</td>\n      <td>0.923385</td>\n      <td>0.920661</td>\n      <td>0.903200</td>\n      <td>0.909700</td>\n      <td>0.012490</td>\n    </tr>\n    <tr>\n      <th>neuralnet train</th>\n      <td>0.726723</td>\n      <td>0.730062</td>\n      <td>0.737216</td>\n      <td>0.727451</td>\n      <td>0.728118</td>\n      <td>0.729914</td>\n      <td>0.004266</td>\n    </tr>\n    <tr>\n      <th>neuralnet val</th>\n      <td>0.819689</td>\n      <td>0.811171</td>\n      <td>0.827034</td>\n      <td>0.812550</td>\n      <td>0.810170</td>\n      <td>0.816123</td>\n      <td>0.007150</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "power_tr_model_errors = pd.concat([baseline_errors, linreg_errors, forestreg_errors, linsvr_errors, neuralnet_errors])\n",
    "power_tr_model_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_models(train_X, train_y):\n",
    "    baseline_errors = cross_val_rmse(baseline_model, train_X, train_y, cv=5, model_name='baseline')\n",
    "    linreg_errors = cross_val_rmse(linreg_model, train_X, train_y, cv=5, model_name='linreg')\n",
    "    forestreg_errors = cross_val_rmse(forestreg_model, train_X, train_y, cv=5, model_name='forestreg')\n",
    "    linsvr_errors = cross_val_rmse(linsvr_model, train_X, train_y, cv=5, model_name='linsvr')\n",
    "\n",
    "    neuralnet_model = create_neuralnet_model(train_X.shape[1:])\n",
    "    neuralnet_errors = cross_val_rmse(neuralnet_model, train_X, train_y, cv=5, model_name='neuralnet')\n",
    "    \n",
    "    return pd.concat([baseline_errors, linreg_errors, forestreg_errors, linsvr_errors, neuralnet_errors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_kNtXOirf8Q"
   },
   "outputs": [],
   "source": [
    "train_X, train_y = separate_features_targets(training_set)\n",
    "\n",
    "# preprocess training features (standardization)\n",
    "feature_preprocessor_std = FeaturePreprocessor(add_combinations=True, std_scale_num=True, onehot_type=True)\n",
    "train_X_std = feature_preprocessor_std.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "j_pXrqpCv6ei",
    "outputId": "c4975eca-0bf3-4b51-e1c5-dd83c13821fd",
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=========] - 0s 15us/step - loss: 0.6259\nEpoch 70/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6213\nEpoch 71/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6305\nEpoch 72/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6203\nEpoch 73/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6452\nEpoch 74/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6322\nEpoch 75/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6211\nEpoch 76/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6179\nEpoch 77/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6186\nEpoch 78/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6161\nEpoch 79/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6197\nEpoch 80/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6181\nEpoch 81/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6190\nEpoch 82/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.6210\nEpoch 83/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6148\nEpoch 84/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6123\nEpoch 85/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6162\nEpoch 86/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6119\nEpoch 87/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6174\nEpoch 88/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6131\nEpoch 89/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.6139\nEpoch 90/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6060\nEpoch 91/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6077\nEpoch 92/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6121\nEpoch 93/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6080\nEpoch 94/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6048\nEpoch 95/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6062\nEpoch 96/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6029\nEpoch 97/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6069\nEpoch 98/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6093\nEpoch 99/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6050\nEpoch 100/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6068\nEpoch 101/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6057\nEpoch 102/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6186\nEpoch 103/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.6200\nEpoch 104/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6049\nEpoch 105/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5993\nEpoch 106/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5968\nEpoch 107/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5976\nEpoch 108/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6000\nEpoch 109/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6010\nEpoch 110/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.5978\nEpoch 111/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5972\nEpoch 112/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5943\nEpoch 113/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5960\nEpoch 114/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5951\nEpoch 115/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5972\nEpoch 116/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5933\nEpoch 117/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5954\nEpoch 118/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5960\nEpoch 119/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6095\nEpoch 120/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5982\nEpoch 121/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5918\nEpoch 122/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5937\nEpoch 123/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6033\nEpoch 124/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6139\nEpoch 125/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5923\nEpoch 126/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5923\nEpoch 127/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5909\nEpoch 128/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5927\nEpoch 129/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5888\nEpoch 130/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5882\nEpoch 131/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5932\nEpoch 132/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5885\nEpoch 133/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5889\nEpoch 134/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5871\nEpoch 135/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5870\nEpoch 136/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5871\nEpoch 137/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5854\nEpoch 138/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5918\nEpoch 139/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5883\nEpoch 140/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5840\nEpoch 141/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5853\nEpoch 142/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5832\nEpoch 143/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5877\nEpoch 144/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5847\nEpoch 145/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5845\nEpoch 146/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5880\nEpoch 147/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5871\nEpoch 148/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5828\nEpoch 149/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5932\nEpoch 150/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5928\nEpoch 1/150\n8672/8672 [==============================] - 0s 37us/step - loss: 2.4493\nEpoch 2/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.9456\nEpoch 3/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.8678\nEpoch 4/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.8435\nEpoch 5/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.8095\nEpoch 6/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.7943\nEpoch 7/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.8073\nEpoch 8/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.7864\nEpoch 9/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.7657\nEpoch 10/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.7673\nEpoch 11/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.7597\nEpoch 12/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.7477\nEpoch 13/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.7467\nEpoch 14/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.8071\nEpoch 15/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.7421\nEpoch 16/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.7338\nEpoch 17/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.7289\nEpoch 18/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.7188\nEpoch 19/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.7397\nEpoch 20/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.7293\nEpoch 21/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.7074\nEpoch 22/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.7324\nEpoch 23/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.7006\nEpoch 24/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.7049\nEpoch 25/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.7342\nEpoch 26/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6950\nEpoch 27/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6870\nEpoch 28/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6923\nEpoch 29/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.7010\nEpoch 30/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6855\nEpoch 31/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6880\nEpoch 32/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6975\nEpoch 33/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6718\nEpoch 34/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6836\nEpoch 35/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6802\nEpoch 36/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6729\nEpoch 37/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6744\nEpoch 38/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6654\nEpoch 39/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6641\nEpoch 40/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6678\nEpoch 41/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6736\nEpoch 42/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6877\nEpoch 43/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6900\nEpoch 44/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6588\nEpoch 45/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6590\nEpoch 46/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6579\nEpoch 47/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6659\nEpoch 48/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6628\nEpoch 49/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6792\nEpoch 50/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6516\nEpoch 51/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6517\nEpoch 52/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6496\nEpoch 53/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6512\nEpoch 54/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6501\nEpoch 55/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6664\nEpoch 56/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6487\nEpoch 57/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6542\nEpoch 58/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6413\nEpoch 59/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6545\nEpoch 60/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6710\nEpoch 61/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6422\nEpoch 62/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6649\nEpoch 63/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.7154\nEpoch 64/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6514\nEpoch 65/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6435\nEpoch 66/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6405\nEpoch 67/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6448\nEpoch 68/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.7018\nEpoch 69/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6391\nEpoch 70/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6416\nEpoch 71/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6513\nEpoch 72/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6368\nEpoch 73/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6384\nEpoch 74/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6302\nEpoch 75/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6306\nEpoch 76/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6315\nEpoch 77/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6312\nEpoch 78/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6329\nEpoch 79/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6290\nEpoch 80/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6272\nEpoch 81/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6269\nEpoch 82/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6256\nEpoch 83/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6221\nEpoch 84/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6281\nEpoch 85/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6292\nEpoch 86/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6249\nEpoch 87/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6291\nEpoch 88/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6196\nEpoch 89/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6262\nEpoch 90/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6252\nEpoch 91/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6324\nEpoch 92/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.6260\nEpoch 93/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.6257\nEpoch 94/150\n8672/8672 [==============================] - 0s 23us/step - loss: 0.6235\nEpoch 95/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.6320\nEpoch 96/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6166\nEpoch 97/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6204\nEpoch 98/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6241\nEpoch 99/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6181\nEpoch 100/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6468\nEpoch 101/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6134\nEpoch 102/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6162\nEpoch 103/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6120\nEpoch 104/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6201\nEpoch 105/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6312\nEpoch 106/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6150\nEpoch 107/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6288\nEpoch 108/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6152\nEpoch 109/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6198\nEpoch 110/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.7094\nEpoch 111/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6384\nEpoch 112/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6410\nEpoch 113/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6406\nEpoch 114/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6239\nEpoch 115/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6299\nEpoch 116/150\n8672/8672 [==============================] - 0s 22us/step - loss: 0.6075\nEpoch 117/150\n8672/8672 [==============================] - 0s 21us/step - loss: 0.6096\nEpoch 118/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.6062\nEpoch 119/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.6086\nEpoch 120/150\n8672/8672 [==============================] - 0s 22us/step - loss: 0.6173\nEpoch 121/150\n8672/8672 [==============================] - 0s 22us/step - loss: 0.6080\nEpoch 122/150\n8672/8672 [==============================] - 0s 22us/step - loss: 0.6119\nEpoch 123/150\n8672/8672 [==============================] - 0s 21us/step - loss: 0.5996\nEpoch 124/150\n8672/8672 [==============================] - 0s 26us/step - loss: 0.6056\nEpoch 125/150\n8672/8672 [==============================] - 0s 26us/step - loss: 0.6037\nEpoch 126/150\n8672/8672 [==============================] - 0s 23us/step - loss: 0.6013\nEpoch 127/150\n8672/8672 [==============================] - 0s 21us/step - loss: 0.5985\nEpoch 128/150\n8672/8672 [==============================] - 0s 22us/step - loss: 0.6045\nEpoch 129/150\n8672/8672 [==============================] - 0s 22us/step - loss: 0.5983\nEpoch 130/150\n8672/8672 [==============================] - 0s 22us/step - loss: 0.6430\nEpoch 131/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.6004\nEpoch 132/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.6047\nEpoch 133/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5966\nEpoch 134/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.5986\nEpoch 135/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6149\nEpoch 136/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6079\nEpoch 137/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6080\nEpoch 138/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6041\nEpoch 139/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5950\nEpoch 140/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5971\nEpoch 141/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5966\nEpoch 142/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6009\nEpoch 143/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6106\nEpoch 144/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5912\nEpoch 145/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5922\nEpoch 146/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5966\nEpoch 147/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5933\nEpoch 148/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.6019\nEpoch 149/150\n8672/8672 [==============================] - 0s 15us/step - loss: 0.5894\nEpoch 150/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6097\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                   fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\nbaseline train   1.123358  1.121115  1.114294  1.119928  1.122722  1.120283   \nbaseline val     1.107938  1.116999  1.143970  1.121741  1.110553  1.120240   \nlinreg train     0.913938  0.915948  0.906405  0.909648  0.916653  0.912518   \nlinreg val       0.909653  0.900677  0.939456  0.929410  0.911433  0.918126   \nforestreg train  0.352452  0.357309  0.352901  0.359270  0.356300  0.355646   \nforestreg val    0.855975  0.828011  0.852394  0.868915  0.830429  0.847145   \nlinsvr train     0.931163  0.933242  0.923078  0.925309  0.929819  0.928522   \nlinsvr val       0.922091  0.920000  0.954623  0.948079  0.918281  0.932615   \nneuralnet train  0.754513  0.765305  0.760587  0.761416  0.888814  0.786127   \nneuralnet val    0.847024  0.815533  0.854683  0.849999  0.940134  0.861475   \n\n                      std  \nbaseline train   0.003609  \nbaseline val     0.014327  \nlinreg train     0.004373  \nlinreg val       0.015838  \nforestreg train  0.002918  \nforestreg val    0.017500  \nlinsvr train     0.004210  \nlinsvr val       0.017312  \nneuralnet train  0.057534  \nneuralnet val    0.046596  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fold 0</th>\n      <th>fold 1</th>\n      <th>fold 2</th>\n      <th>fold 3</th>\n      <th>fold 4</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>baseline train</th>\n      <td>1.123358</td>\n      <td>1.121115</td>\n      <td>1.114294</td>\n      <td>1.119928</td>\n      <td>1.122722</td>\n      <td>1.120283</td>\n      <td>0.003609</td>\n    </tr>\n    <tr>\n      <th>baseline val</th>\n      <td>1.107938</td>\n      <td>1.116999</td>\n      <td>1.143970</td>\n      <td>1.121741</td>\n      <td>1.110553</td>\n      <td>1.120240</td>\n      <td>0.014327</td>\n    </tr>\n    <tr>\n      <th>linreg train</th>\n      <td>0.913938</td>\n      <td>0.915948</td>\n      <td>0.906405</td>\n      <td>0.909648</td>\n      <td>0.916653</td>\n      <td>0.912518</td>\n      <td>0.004373</td>\n    </tr>\n    <tr>\n      <th>linreg val</th>\n      <td>0.909653</td>\n      <td>0.900677</td>\n      <td>0.939456</td>\n      <td>0.929410</td>\n      <td>0.911433</td>\n      <td>0.918126</td>\n      <td>0.015838</td>\n    </tr>\n    <tr>\n      <th>forestreg train</th>\n      <td>0.352452</td>\n      <td>0.357309</td>\n      <td>0.352901</td>\n      <td>0.359270</td>\n      <td>0.356300</td>\n      <td>0.355646</td>\n      <td>0.002918</td>\n    </tr>\n    <tr>\n      <th>forestreg val</th>\n      <td>0.855975</td>\n      <td>0.828011</td>\n      <td>0.852394</td>\n      <td>0.868915</td>\n      <td>0.830429</td>\n      <td>0.847145</td>\n      <td>0.017500</td>\n    </tr>\n    <tr>\n      <th>linsvr train</th>\n      <td>0.931163</td>\n      <td>0.933242</td>\n      <td>0.923078</td>\n      <td>0.925309</td>\n      <td>0.929819</td>\n      <td>0.928522</td>\n      <td>0.004210</td>\n    </tr>\n    <tr>\n      <th>linsvr val</th>\n      <td>0.922091</td>\n      <td>0.920000</td>\n      <td>0.954623</td>\n      <td>0.948079</td>\n      <td>0.918281</td>\n      <td>0.932615</td>\n      <td>0.017312</td>\n    </tr>\n    <tr>\n      <th>neuralnet train</th>\n      <td>0.754513</td>\n      <td>0.765305</td>\n      <td>0.760587</td>\n      <td>0.761416</td>\n      <td>0.888814</td>\n      <td>0.786127</td>\n      <td>0.057534</td>\n    </tr>\n    <tr>\n      <th>neuralnet val</th>\n      <td>0.847024</td>\n      <td>0.815533</td>\n      <td>0.854683</td>\n      <td>0.849999</td>\n      <td>0.940134</td>\n      <td>0.861475</td>\n      <td>0.046596</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "std_errors = evaluate_models(train_X_std, train_y)\n",
    "std_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "UhAzmaTKoYYp",
    "outputId": "891c79ed-9fa3-4109-fe84-8272f84d087d"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n           max_features='auto', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=1, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n           oob_score=False, random_state=None, verbose=0, warm_start=False)"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "train_X, train_y = separate_features_targets(training_set)\n",
    "\n",
    "train_X = feature_preprocessor.fit_transform(train_X)\n",
    "forestreg_model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4ruWR7baoznC",
    "outputId": "3080df9f-ec1f-4263-bd2d-cd6e1a304baa"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               feature  importance\n0                 year    0.138135\n1          ratingCount    0.126447\n2     reviewsPerRating    0.117280\n3             duration    0.095743\n4             nrOfWins    0.078460\n5     type_video.movie    0.076736\n6      nrOfUserReviews    0.043138\n7           nrOfPhotos    0.038654\n8     nrOfNewsArticles    0.038006\n9    winsPerNomination    0.025696\n10         Documentary    0.020118\n11               Drama    0.016692\n12    totalNominations    0.015943\n13           nrOfGenre    0.014991\n14               SciFi    0.014905\n15              Horror    0.013746\n16              Comedy    0.012995\n17           Animation    0.010112\n18       type_video.tv    0.009749\n19              Family    0.008361\n20     nrOfNominations    0.008039\n21                News    0.007365\n22              Action    0.007108\n23           RealityTV    0.007052\n24             Fantasy    0.006416\n25             Romance    0.006156\n26           Adventure    0.005302\n27  type_video.episode    0.004656\n28               Crime    0.004322\n29             Musical    0.004315\n30               Music    0.004068\n31            Thriller    0.003726\n32            TalkShow    0.003646\n33               Short    0.002142\n34             Mystery    0.001997\n35           Biography    0.001616\n36             History    0.001397\n37                 War    0.000992\n38             Western    0.000972\n39            GameShow    0.000936\n40               Sport    0.000885\n41               Adult    0.000556\n42           type_game    0.000309\n43            FilmNoir    0.000123",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>year</td>\n      <td>0.138135</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ratingCount</td>\n      <td>0.126447</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>reviewsPerRating</td>\n      <td>0.117280</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>duration</td>\n      <td>0.095743</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>nrOfWins</td>\n      <td>0.078460</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>type_video.movie</td>\n      <td>0.076736</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>nrOfUserReviews</td>\n      <td>0.043138</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>nrOfPhotos</td>\n      <td>0.038654</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>nrOfNewsArticles</td>\n      <td>0.038006</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>winsPerNomination</td>\n      <td>0.025696</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Documentary</td>\n      <td>0.020118</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Drama</td>\n      <td>0.016692</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>totalNominations</td>\n      <td>0.015943</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>nrOfGenre</td>\n      <td>0.014991</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>SciFi</td>\n      <td>0.014905</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Horror</td>\n      <td>0.013746</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Comedy</td>\n      <td>0.012995</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Animation</td>\n      <td>0.010112</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>type_video.tv</td>\n      <td>0.009749</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Family</td>\n      <td>0.008361</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>nrOfNominations</td>\n      <td>0.008039</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>News</td>\n      <td>0.007365</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Action</td>\n      <td>0.007108</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>RealityTV</td>\n      <td>0.007052</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Fantasy</td>\n      <td>0.006416</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Romance</td>\n      <td>0.006156</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>Adventure</td>\n      <td>0.005302</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>type_video.episode</td>\n      <td>0.004656</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>Crime</td>\n      <td>0.004322</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>Musical</td>\n      <td>0.004315</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>Music</td>\n      <td>0.004068</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>Thriller</td>\n      <td>0.003726</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>TalkShow</td>\n      <td>0.003646</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>Short</td>\n      <td>0.002142</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>Mystery</td>\n      <td>0.001997</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>Biography</td>\n      <td>0.001616</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>History</td>\n      <td>0.001397</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>War</td>\n      <td>0.000992</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>Western</td>\n      <td>0.000972</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>GameShow</td>\n      <td>0.000936</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>Sport</td>\n      <td>0.000885</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>Adult</td>\n      <td>0.000556</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>type_game</td>\n      <td>0.000309</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>FilmNoir</td>\n      <td>0.000123</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "forestreg_feature_importances = pd.DataFrame({'feature': train_X.columns, 'importance': forestreg_model.feature_importances_})\n",
    "forestreg_feature_importances = forestreg_feature_importances.sort_values(by='importance', ascending=False)\n",
    "forestreg_feature_importances = forestreg_feature_importances.reset_index(drop=True)\n",
    "forestreg_feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "VijpKF9a1SiM",
    "outputId": "592441ba-d9f9-49fc-da01-c1eecf92dfc6"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Mystery',\n 'Biography',\n 'History',\n 'War',\n 'Western',\n 'GameShow',\n 'Sport',\n 'Adult',\n 'type_game',\n 'FilmNoir']"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "least_important_features = list(forestreg_feature_importances.iloc[-10:]['feature'].values)\n",
    "least_important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "70cpd-6nzqpB",
    "outputId": "465b443a-fe45-4e53-9ac9-1b152c64a89a"
   },
   "outputs": [],
   "source": [
    "train_X, train_y = separate_features_targets(training_set)\n",
    "\n",
    "# preprocess training features (power transform, remove least important features)\n",
    "feature_preprocessor = FeaturePreprocessor(add_combinations=True, powertransform_num=True, onehot_type=True,\n",
    "                                           drop_features=least_important_features)\n",
    "train_X = feature_preprocessor.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "pM36l8Cg3i0v",
    "outputId": "2533f3fa-0c3b-4200-a129-b6fe95c5589f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10840 entries, 11613 to 14254\nData columns (total 34 columns):\nratingCount           10840 non-null float64\nduration              10840 non-null float64\nyear                  10840 non-null float64\nnrOfWins              10840 non-null float64\nnrOfNominations       10840 non-null float64\nnrOfPhotos            10840 non-null float64\nnrOfNewsArticles      10840 non-null float64\nnrOfUserReviews       10840 non-null float64\nnrOfGenre             10840 non-null float64\ntotalNominations      10840 non-null float64\nwinsPerNomination     10840 non-null float64\nreviewsPerRating      10840 non-null float64\ntype_video.episode    10840 non-null float64\ntype_video.movie      10840 non-null float64\ntype_video.tv         10840 non-null float64\nAction                10840 non-null int64\nAdventure             10840 non-null int64\nAnimation             10840 non-null int64\nComedy                10840 non-null int64\nCrime                 10840 non-null int64\nDocumentary           10840 non-null int64\nDrama                 10840 non-null int64\nFamily                10840 non-null int64\nFantasy               10840 non-null int64\nHorror                10840 non-null int64\nMusic                 10840 non-null int64\nMusical               10840 non-null int64\nNews                  10840 non-null int64\nRealityTV             10840 non-null int64\nRomance               10840 non-null int64\nSciFi                 10840 non-null int64\nShort                 10840 non-null int64\nTalkShow              10840 non-null int64\nThriller              10840 non-null int64\ndtypes: float64(15), int64(19)\nmemory usage: 2.9 MB\n"
    }
   ],
   "source": [
    "train_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "K2w1K7640I42",
    "outputId": "ba9f2b98-9cb7-49ac-f88d-ca510a7dfc5c",
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=========] - 0s 17us/step - loss: 0.6064\nEpoch 70/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6044\nEpoch 71/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6055\nEpoch 72/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6053\nEpoch 73/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6023\nEpoch 74/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6045\nEpoch 75/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6046\nEpoch 76/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6019\nEpoch 77/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6012\nEpoch 78/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6046\nEpoch 79/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6014\nEpoch 80/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6012\nEpoch 81/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5988\nEpoch 82/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5998\nEpoch 83/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5982\nEpoch 84/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5959\nEpoch 85/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5960\nEpoch 86/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5998\nEpoch 87/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5980\nEpoch 88/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5960\nEpoch 89/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5964\nEpoch 90/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5948\nEpoch 91/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5954\nEpoch 92/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5942\nEpoch 93/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5950\nEpoch 94/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5927\nEpoch 95/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5917\nEpoch 96/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5936\nEpoch 97/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5932\nEpoch 98/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5939\nEpoch 99/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5914\nEpoch 100/150\n8672/8672 [==============================] - 0s 22us/step - loss: 0.5899\nEpoch 101/150\n8672/8672 [==============================] - 0s 23us/step - loss: 0.5902\nEpoch 102/150\n8672/8672 [==============================] - 0s 21us/step - loss: 0.5906\nEpoch 103/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.5887\nEpoch 104/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.5911\nEpoch 105/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5892\nEpoch 106/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5899\nEpoch 107/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5884\nEpoch 108/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5882\nEpoch 109/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5884\nEpoch 110/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5887\nEpoch 111/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5869\nEpoch 112/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5869\nEpoch 113/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5854\nEpoch 114/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5852\nEpoch 115/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5840\nEpoch 116/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5833\nEpoch 117/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5807\nEpoch 118/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5828\nEpoch 119/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5841\nEpoch 120/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5811\nEpoch 121/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5818\nEpoch 122/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5812\nEpoch 123/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5818\nEpoch 124/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5790\nEpoch 125/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5806\nEpoch 126/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5802\nEpoch 127/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5762\nEpoch 128/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5787\nEpoch 129/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5776\nEpoch 130/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5769\nEpoch 131/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5762\nEpoch 132/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5762\nEpoch 133/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5758\nEpoch 134/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5772\nEpoch 135/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5740\nEpoch 136/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5737\nEpoch 137/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5750\nEpoch 138/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5749\nEpoch 139/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5745\nEpoch 140/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5731\nEpoch 141/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5738\nEpoch 142/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5711\nEpoch 143/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5704\nEpoch 144/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5725\nEpoch 145/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5708\nEpoch 146/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5727\nEpoch 147/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5695\nEpoch 148/150\n8672/8672 [==============================] - 0s 21us/step - loss: 0.5709\nEpoch 149/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5677\nEpoch 150/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5691\nEpoch 1/150\n8672/8672 [==============================] - 0s 47us/step - loss: 1.8741\nEpoch 2/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.8990\nEpoch 3/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.8329\nEpoch 4/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.7964\nEpoch 5/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.7720\nEpoch 6/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.7542\nEpoch 7/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.7413\nEpoch 8/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.7313\nEpoch 9/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.7220\nEpoch 10/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.7159\nEpoch 11/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.7060\nEpoch 12/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6996\nEpoch 13/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6964\nEpoch 14/150\n8672/8672 [==============================] - 0s 22us/step - loss: 0.6879\nEpoch 15/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.6858\nEpoch 16/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.6829\nEpoch 17/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6747\nEpoch 18/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6728\nEpoch 19/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6692\nEpoch 20/150\n8672/8672 [==============================] - 0s 22us/step - loss: 0.6678\nEpoch 21/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6657\nEpoch 22/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6611\nEpoch 23/150\n8672/8672 [==============================] - 0s 21us/step - loss: 0.6593\nEpoch 24/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.6565\nEpoch 25/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.6548\nEpoch 26/150\n8672/8672 [==============================] - 0s 21us/step - loss: 0.6538\nEpoch 27/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6504\nEpoch 28/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6488\nEpoch 29/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6469\nEpoch 30/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6438\nEpoch 31/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6422\nEpoch 32/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6399\nEpoch 33/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6400\nEpoch 34/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.6390\nEpoch 35/150\n8672/8672 [==============================] - 0s 21us/step - loss: 0.6385\nEpoch 36/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.6365\nEpoch 37/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.6342\nEpoch 38/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6330\nEpoch 39/150\n8672/8672 [==============================] - 0s 22us/step - loss: 0.6326\nEpoch 40/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6306\nEpoch 41/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6296\nEpoch 42/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.6272\nEpoch 43/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6297\nEpoch 44/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6242\nEpoch 45/150\n8672/8672 [==============================] - 0s 21us/step - loss: 0.6238\nEpoch 46/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.6246\nEpoch 47/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.6239\nEpoch 48/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6228\nEpoch 49/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6221\nEpoch 50/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6189\nEpoch 51/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6200\nEpoch 52/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6189\nEpoch 53/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6175\nEpoch 54/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6177\nEpoch 55/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6145\nEpoch 56/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6138\nEpoch 57/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6182\nEpoch 58/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.6130\nEpoch 59/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6141\nEpoch 60/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6117\nEpoch 61/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6106\nEpoch 62/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6074\nEpoch 63/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6098\nEpoch 64/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6080\nEpoch 65/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6078\nEpoch 66/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6071\nEpoch 67/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.6063\nEpoch 68/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.6041\nEpoch 69/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6045\nEpoch 70/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.6043\nEpoch 71/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.6027\nEpoch 72/150\n8672/8672 [==============================] - 0s 21us/step - loss: 0.6024\nEpoch 73/150\n8672/8672 [==============================] - 0s 22us/step - loss: 0.6023\nEpoch 74/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.6041\nEpoch 75/150\n8672/8672 [==============================] - 0s 27us/step - loss: 0.6025\nEpoch 76/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5989\nEpoch 77/150\n8672/8672 [==============================] - 0s 21us/step - loss: 0.5982\nEpoch 78/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5969\nEpoch 79/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5981\nEpoch 80/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5953\nEpoch 81/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5978\nEpoch 82/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5955\nEpoch 83/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5951\nEpoch 84/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5948\nEpoch 85/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5924\nEpoch 86/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5948\nEpoch 87/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5923\nEpoch 88/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5922\nEpoch 89/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5912\nEpoch 90/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5918\nEpoch 91/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5918\nEpoch 92/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5904\nEpoch 93/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5890\nEpoch 94/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5887\nEpoch 95/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5879\nEpoch 96/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5862\nEpoch 97/150\n8672/8672 [==============================] - 0s 25us/step - loss: 0.5874\nEpoch 98/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5866\nEpoch 99/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5846\nEpoch 100/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5838\nEpoch 101/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5863\nEpoch 102/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5843\nEpoch 103/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5862\nEpoch 104/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5835\nEpoch 105/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5835\nEpoch 106/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5819\nEpoch 107/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5801\nEpoch 108/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5807\nEpoch 109/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5825\nEpoch 110/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5794\nEpoch 111/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5826\nEpoch 112/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5797\nEpoch 113/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5822\nEpoch 114/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5798\nEpoch 115/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5790\nEpoch 116/150\n8672/8672 [==============================] - 0s 22us/step - loss: 0.5786\nEpoch 117/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5785\nEpoch 118/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5768\nEpoch 119/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5767\nEpoch 120/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.5757\nEpoch 121/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5765\nEpoch 122/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.5738\nEpoch 123/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5780\nEpoch 124/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5741\nEpoch 125/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5732\nEpoch 126/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5706\nEpoch 127/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5720\nEpoch 128/150\n8672/8672 [==============================] - 0s 21us/step - loss: 0.5735\nEpoch 129/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5712\nEpoch 130/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5714\nEpoch 131/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5723\nEpoch 132/150\n8672/8672 [==============================] - 0s 16us/step - loss: 0.5725\nEpoch 133/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5714\nEpoch 134/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5706\nEpoch 135/150\n8672/8672 [==============================] - 0s 21us/step - loss: 0.5710\nEpoch 136/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5711\nEpoch 137/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5707\nEpoch 138/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5692\nEpoch 139/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5702\nEpoch 140/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5666\nEpoch 141/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5681\nEpoch 142/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5692\nEpoch 143/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5686\nEpoch 144/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5686\nEpoch 145/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5669\nEpoch 146/150\n8672/8672 [==============================] - 0s 17us/step - loss: 0.5658\nEpoch 147/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5672\nEpoch 148/150\n8672/8672 [==============================] - 0s 19us/step - loss: 0.5680\nEpoch 149/150\n8672/8672 [==============================] - 0s 20us/step - loss: 0.5634\nEpoch 150/150\n8672/8672 [==============================] - 0s 18us/step - loss: 0.5638\n"
    }
   ],
   "source": [
    "power_tr_drop_errors = evaluate_models(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "colab_type": "code",
    "id": "z2yugWWN0YRF",
    "outputId": "8e79b269-3acf-484b-afcd-ac7259654f5e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Standardization Only\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                   fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\nbaseline train   1.123358  1.121115  1.114294  1.119928  1.122722  1.120283   \nbaseline val     1.107938  1.116999  1.143970  1.121741  1.110553  1.120240   \nlinreg train     0.913938  0.915948  0.906405  0.909648  0.916653  0.912518   \nlinreg val       0.909653  0.900677  0.939456  0.929410  0.911433  0.918126   \nforestreg train  0.352452  0.357309  0.352901  0.359270  0.356300  0.355646   \nforestreg val    0.855975  0.828011  0.852394  0.868915  0.830429  0.847145   \nlinsvr train     0.931163  0.933242  0.923078  0.925309  0.929819  0.928522   \nlinsvr val       0.922091  0.920000  0.954623  0.948079  0.918281  0.932615   \nneuralnet train  0.754513  0.765305  0.760587  0.761416  0.888814  0.786127   \nneuralnet val    0.847024  0.815533  0.854683  0.849999  0.940134  0.861475   \n\n                      std  \nbaseline train   0.003609  \nbaseline val     0.014327  \nlinreg train     0.004373  \nlinreg val       0.015838  \nforestreg train  0.002918  \nforestreg val    0.017500  \nlinsvr train     0.004210  \nlinsvr val       0.017312  \nneuralnet train  0.057534  \nneuralnet val    0.046596  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fold 0</th>\n      <th>fold 1</th>\n      <th>fold 2</th>\n      <th>fold 3</th>\n      <th>fold 4</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>baseline train</th>\n      <td>1.123358</td>\n      <td>1.121115</td>\n      <td>1.114294</td>\n      <td>1.119928</td>\n      <td>1.122722</td>\n      <td>1.120283</td>\n      <td>0.003609</td>\n    </tr>\n    <tr>\n      <th>baseline val</th>\n      <td>1.107938</td>\n      <td>1.116999</td>\n      <td>1.143970</td>\n      <td>1.121741</td>\n      <td>1.110553</td>\n      <td>1.120240</td>\n      <td>0.014327</td>\n    </tr>\n    <tr>\n      <th>linreg train</th>\n      <td>0.913938</td>\n      <td>0.915948</td>\n      <td>0.906405</td>\n      <td>0.909648</td>\n      <td>0.916653</td>\n      <td>0.912518</td>\n      <td>0.004373</td>\n    </tr>\n    <tr>\n      <th>linreg val</th>\n      <td>0.909653</td>\n      <td>0.900677</td>\n      <td>0.939456</td>\n      <td>0.929410</td>\n      <td>0.911433</td>\n      <td>0.918126</td>\n      <td>0.015838</td>\n    </tr>\n    <tr>\n      <th>forestreg train</th>\n      <td>0.352452</td>\n      <td>0.357309</td>\n      <td>0.352901</td>\n      <td>0.359270</td>\n      <td>0.356300</td>\n      <td>0.355646</td>\n      <td>0.002918</td>\n    </tr>\n    <tr>\n      <th>forestreg val</th>\n      <td>0.855975</td>\n      <td>0.828011</td>\n      <td>0.852394</td>\n      <td>0.868915</td>\n      <td>0.830429</td>\n      <td>0.847145</td>\n      <td>0.017500</td>\n    </tr>\n    <tr>\n      <th>linsvr train</th>\n      <td>0.931163</td>\n      <td>0.933242</td>\n      <td>0.923078</td>\n      <td>0.925309</td>\n      <td>0.929819</td>\n      <td>0.928522</td>\n      <td>0.004210</td>\n    </tr>\n    <tr>\n      <th>linsvr val</th>\n      <td>0.922091</td>\n      <td>0.920000</td>\n      <td>0.954623</td>\n      <td>0.948079</td>\n      <td>0.918281</td>\n      <td>0.932615</td>\n      <td>0.017312</td>\n    </tr>\n    <tr>\n      <th>neuralnet train</th>\n      <td>0.754513</td>\n      <td>0.765305</td>\n      <td>0.760587</td>\n      <td>0.761416</td>\n      <td>0.888814</td>\n      <td>0.786127</td>\n      <td>0.057534</td>\n    </tr>\n    <tr>\n      <th>neuralnet val</th>\n      <td>0.847024</td>\n      <td>0.815533</td>\n      <td>0.854683</td>\n      <td>0.849999</td>\n      <td>0.940134</td>\n      <td>0.861475</td>\n      <td>0.046596</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\nPower Transform Only\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                   fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\nbaseline train   1.123358  1.121115  1.114294  1.119928  1.122722  1.120283   \nbaseline val     1.107938  1.116999  1.143970  1.121741  1.110553  1.120240   \nlinreg train     0.893267  0.896926  0.889980  0.890549  0.894051  0.892955   \nlinreg val       0.897161  0.881527  0.910644  0.908679  0.894478  0.898498   \nforestreg train  0.348015  0.354293  0.351649  0.364987  0.350212  0.353831   \nforestreg val    0.858008  0.847944  0.852273  0.844156  0.828591  0.846194   \nlinsvr train     0.906312  0.911642  0.903500  0.902954  0.907608  0.906403   \nlinsvr val       0.908033  0.893220  0.923385  0.920661  0.903200  0.909700   \nneuralnet train  0.726723  0.730062  0.737216  0.727451  0.728118  0.729914   \nneuralnet val    0.819689  0.811171  0.827034  0.812550  0.810170  0.816123   \n\n                      std  \nbaseline train   0.003609  \nbaseline val     0.014327  \nlinreg train     0.002815  \nlinreg val       0.011802  \nforestreg train  0.006640  \nforestreg val    0.011109  \nlinsvr train     0.003508  \nlinsvr val       0.012490  \nneuralnet train  0.004266  \nneuralnet val    0.007150  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fold 0</th>\n      <th>fold 1</th>\n      <th>fold 2</th>\n      <th>fold 3</th>\n      <th>fold 4</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>baseline train</th>\n      <td>1.123358</td>\n      <td>1.121115</td>\n      <td>1.114294</td>\n      <td>1.119928</td>\n      <td>1.122722</td>\n      <td>1.120283</td>\n      <td>0.003609</td>\n    </tr>\n    <tr>\n      <th>baseline val</th>\n      <td>1.107938</td>\n      <td>1.116999</td>\n      <td>1.143970</td>\n      <td>1.121741</td>\n      <td>1.110553</td>\n      <td>1.120240</td>\n      <td>0.014327</td>\n    </tr>\n    <tr>\n      <th>linreg train</th>\n      <td>0.893267</td>\n      <td>0.896926</td>\n      <td>0.889980</td>\n      <td>0.890549</td>\n      <td>0.894051</td>\n      <td>0.892955</td>\n      <td>0.002815</td>\n    </tr>\n    <tr>\n      <th>linreg val</th>\n      <td>0.897161</td>\n      <td>0.881527</td>\n      <td>0.910644</td>\n      <td>0.908679</td>\n      <td>0.894478</td>\n      <td>0.898498</td>\n      <td>0.011802</td>\n    </tr>\n    <tr>\n      <th>forestreg train</th>\n      <td>0.348015</td>\n      <td>0.354293</td>\n      <td>0.351649</td>\n      <td>0.364987</td>\n      <td>0.350212</td>\n      <td>0.353831</td>\n      <td>0.006640</td>\n    </tr>\n    <tr>\n      <th>forestreg val</th>\n      <td>0.858008</td>\n      <td>0.847944</td>\n      <td>0.852273</td>\n      <td>0.844156</td>\n      <td>0.828591</td>\n      <td>0.846194</td>\n      <td>0.011109</td>\n    </tr>\n    <tr>\n      <th>linsvr train</th>\n      <td>0.906312</td>\n      <td>0.911642</td>\n      <td>0.903500</td>\n      <td>0.902954</td>\n      <td>0.907608</td>\n      <td>0.906403</td>\n      <td>0.003508</td>\n    </tr>\n    <tr>\n      <th>linsvr val</th>\n      <td>0.908033</td>\n      <td>0.893220</td>\n      <td>0.923385</td>\n      <td>0.920661</td>\n      <td>0.903200</td>\n      <td>0.909700</td>\n      <td>0.012490</td>\n    </tr>\n    <tr>\n      <th>neuralnet train</th>\n      <td>0.726723</td>\n      <td>0.730062</td>\n      <td>0.737216</td>\n      <td>0.727451</td>\n      <td>0.728118</td>\n      <td>0.729914</td>\n      <td>0.004266</td>\n    </tr>\n    <tr>\n      <th>neuralnet val</th>\n      <td>0.819689</td>\n      <td>0.811171</td>\n      <td>0.827034</td>\n      <td>0.812550</td>\n      <td>0.810170</td>\n      <td>0.816123</td>\n      <td>0.007150</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\nPower Transform and Drop 10 Least Important Features\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                   fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\nbaseline train   1.123358  1.121115  1.114294  1.119928  1.122722  1.120283   \nbaseline val     1.107938  1.116999  1.143970  1.121741  1.110553  1.120240   \nlinreg train     0.894511  0.898298  0.891318  0.891449  0.895522  0.894220   \nlinreg val       0.897661  0.881696  0.910642  0.910051  0.893674  0.898745   \nforestreg train  0.352870  0.361841  0.353304  0.364521  0.354510  0.357409   \nforestreg val    0.844424  0.836910  0.854385  0.845759  0.823655  0.841027   \nlinsvr train     0.907547  0.913225  0.904705  0.903970  0.908863  0.907662   \nlinsvr val       0.908602  0.894419  0.923609  0.921272  0.904178  0.910416   \nneuralnet train  0.755310  0.748043  0.740307  0.751168  0.744720  0.747909   \nneuralnet val    0.824819  0.807315  0.827559  0.832562  0.826229  0.823697   \n\n                      std  \nbaseline train   0.003609  \nbaseline val     0.014327  \nlinreg train     0.002937  \nlinreg val       0.012113  \nforestreg train  0.005387  \nforestreg val    0.011523  \nlinsvr train     0.003701  \nlinsvr val       0.012145  \nneuralnet train  0.005774  \nneuralnet val    0.009612  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fold 0</th>\n      <th>fold 1</th>\n      <th>fold 2</th>\n      <th>fold 3</th>\n      <th>fold 4</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>baseline train</th>\n      <td>1.123358</td>\n      <td>1.121115</td>\n      <td>1.114294</td>\n      <td>1.119928</td>\n      <td>1.122722</td>\n      <td>1.120283</td>\n      <td>0.003609</td>\n    </tr>\n    <tr>\n      <th>baseline val</th>\n      <td>1.107938</td>\n      <td>1.116999</td>\n      <td>1.143970</td>\n      <td>1.121741</td>\n      <td>1.110553</td>\n      <td>1.120240</td>\n      <td>0.014327</td>\n    </tr>\n    <tr>\n      <th>linreg train</th>\n      <td>0.894511</td>\n      <td>0.898298</td>\n      <td>0.891318</td>\n      <td>0.891449</td>\n      <td>0.895522</td>\n      <td>0.894220</td>\n      <td>0.002937</td>\n    </tr>\n    <tr>\n      <th>linreg val</th>\n      <td>0.897661</td>\n      <td>0.881696</td>\n      <td>0.910642</td>\n      <td>0.910051</td>\n      <td>0.893674</td>\n      <td>0.898745</td>\n      <td>0.012113</td>\n    </tr>\n    <tr>\n      <th>forestreg train</th>\n      <td>0.352870</td>\n      <td>0.361841</td>\n      <td>0.353304</td>\n      <td>0.364521</td>\n      <td>0.354510</td>\n      <td>0.357409</td>\n      <td>0.005387</td>\n    </tr>\n    <tr>\n      <th>forestreg val</th>\n      <td>0.844424</td>\n      <td>0.836910</td>\n      <td>0.854385</td>\n      <td>0.845759</td>\n      <td>0.823655</td>\n      <td>0.841027</td>\n      <td>0.011523</td>\n    </tr>\n    <tr>\n      <th>linsvr train</th>\n      <td>0.907547</td>\n      <td>0.913225</td>\n      <td>0.904705</td>\n      <td>0.903970</td>\n      <td>0.908863</td>\n      <td>0.907662</td>\n      <td>0.003701</td>\n    </tr>\n    <tr>\n      <th>linsvr val</th>\n      <td>0.908602</td>\n      <td>0.894419</td>\n      <td>0.923609</td>\n      <td>0.921272</td>\n      <td>0.904178</td>\n      <td>0.910416</td>\n      <td>0.012145</td>\n    </tr>\n    <tr>\n      <th>neuralnet train</th>\n      <td>0.755310</td>\n      <td>0.748043</td>\n      <td>0.740307</td>\n      <td>0.751168</td>\n      <td>0.744720</td>\n      <td>0.747909</td>\n      <td>0.005774</td>\n    </tr>\n    <tr>\n      <th>neuralnet val</th>\n      <td>0.824819</td>\n      <td>0.807315</td>\n      <td>0.827559</td>\n      <td>0.832562</td>\n      <td>0.826229</td>\n      <td>0.823697</td>\n      <td>0.009612</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "print(\"Standardization Only\")\n",
    "display(std_errors)\n",
    "\n",
    "print(\"\\n\\nPower Transform Only\")\n",
    "display(power_tr_model_errors)\n",
    "\n",
    "print(\"\\n\\nPower Transform and Drop 10 Least Important Features\")\n",
    "display(power_tr_drop_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_g3GFhVmBhfP"
   },
   "source": [
    "Notes about the models so far:\n",
    "- Power transformation to numerical columns results in less error except for the random forest model where the error increases slightly\n",
    "- Removing the least important features results in slightly more error except for the random forest model where error slightly decreases\n",
    "\n",
    "Best models so far:\n",
    "- Random Forest Regressor\n",
    "- Dense Neural Network"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOyG/Y5oofr0RsEDavKQcXE",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "predicting_imdb_ratings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}