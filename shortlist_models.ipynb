{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cgCpIyJuT1Zb"
   },
   "source": [
    "## Shortlist Promising Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes the data has been split into a training and a test set. If not, run get_data.ipynb first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HTOZvrk8Ye-X"
   },
   "source": [
    "1. Try these models:\n",
    "  - Linear Regression\n",
    "  - Random Forest Regressor\n",
    "  - Dense Neural Network\n",
    "  - Linear SVR\n",
    "2. Measure and compare their performance on RMSE (compare means and standard deviations of RMSE for different models as well)\n",
    "3. Make a quick round of feature selection and engineering:\n",
    "  - Try transforming variables to normal distributions\n",
    "  - Try removing unimportant features\n",
    "  - Try adding polynomial features\n",
    "4. Perform one or two more quick iterations of the five previous steps.\n",
    "5. Shortlist the top three to five most promising models, preferring models that\n",
    "make different types of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TRAINING_FILEPATH = 'data/training_set.csv'\n",
    "TEST_FILEPATH = 'data/test_set.csv'\n",
    "\n",
    "training_set = pd.read_csv(TRAINING_FILEPATH, index_col='index')\n",
    "test_set = pd.read_csv(TEST_FILEPATH, index_col='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_utils import separate_features_targets, FeaturePreprocessor\n",
    "\n",
    "train_X, train_y = separate_features_targets(training_set)\n",
    "\n",
    "# preprocess training features: power transform\n",
    "feature_preprocessor = FeaturePreprocessor(add_combinations=True, powertransform_num=True, onehot_type=True)\n",
    "train_X = feature_preprocessor.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rTq8z67lWNxq",
    "outputId": "a9ab2413-b5f3-4273-bd42-69f3ddbf27f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1203423450376466"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the baseline RMSE is the standard deviation of the targets\n",
    "train_y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vv8ClUkjf0tN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold 0</th>\n",
       "      <th>fold 1</th>\n",
       "      <th>fold 2</th>\n",
       "      <th>fold 3</th>\n",
       "      <th>fold 4</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline train</th>\n",
       "      <td>1.123358</td>\n",
       "      <td>1.121115</td>\n",
       "      <td>1.114294</td>\n",
       "      <td>1.119928</td>\n",
       "      <td>1.122722</td>\n",
       "      <td>1.120283</td>\n",
       "      <td>0.003609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline val</th>\n",
       "      <td>1.107938</td>\n",
       "      <td>1.116999</td>\n",
       "      <td>1.143970</td>\n",
       "      <td>1.121741</td>\n",
       "      <td>1.110553</td>\n",
       "      <td>1.120240</td>\n",
       "      <td>0.014327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\n",
       "baseline train  1.123358  1.121115  1.114294  1.119928  1.122722  1.120283   \n",
       "baseline val    1.107938  1.116999  1.143970  1.121741  1.110553  1.120240   \n",
       "\n",
       "                     std  \n",
       "baseline train  0.003609  \n",
       "baseline val    0.014327  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from train_utils import cross_val_rmse\n",
    "\n",
    "baseline_model = DummyRegressor(strategy='mean')\n",
    "baseline_errors = cross_val_rmse(baseline_model, train_X, train_y, cv=5, random_state=42, model_name='baseline')\n",
    "display(baseline_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ru-7KyWcjZuf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold 0</th>\n",
       "      <th>fold 1</th>\n",
       "      <th>fold 2</th>\n",
       "      <th>fold 3</th>\n",
       "      <th>fold 4</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>linreg train</th>\n",
       "      <td>0.893267</td>\n",
       "      <td>0.896926</td>\n",
       "      <td>0.889980</td>\n",
       "      <td>0.890549</td>\n",
       "      <td>0.894051</td>\n",
       "      <td>0.892955</td>\n",
       "      <td>0.002815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linreg val</th>\n",
       "      <td>0.897161</td>\n",
       "      <td>0.881527</td>\n",
       "      <td>0.910644</td>\n",
       "      <td>0.908679</td>\n",
       "      <td>0.894478</td>\n",
       "      <td>0.898498</td>\n",
       "      <td>0.011802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\n",
       "linreg train  0.893267  0.896926  0.889980  0.890549  0.894051  0.892955   \n",
       "linreg val    0.897161  0.881527  0.910644  0.908679  0.894478  0.898498   \n",
       "\n",
       "                   std  \n",
       "linreg train  0.002815  \n",
       "linreg val    0.011802  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg_model = LinearRegression()\n",
    "linreg_errors = cross_val_rmse(linreg_model, train_X, train_y, cv=5, random_state=42, model_name='linreg')\n",
    "display(linreg_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SUv59CtFnpHo"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold 0</th>\n",
       "      <th>fold 1</th>\n",
       "      <th>fold 2</th>\n",
       "      <th>fold 3</th>\n",
       "      <th>fold 4</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>forestreg train</th>\n",
       "      <td>0.348015</td>\n",
       "      <td>0.354293</td>\n",
       "      <td>0.351649</td>\n",
       "      <td>0.364987</td>\n",
       "      <td>0.350212</td>\n",
       "      <td>0.353831</td>\n",
       "      <td>0.006640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forestreg val</th>\n",
       "      <td>0.858008</td>\n",
       "      <td>0.847944</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.828591</td>\n",
       "      <td>0.846194</td>\n",
       "      <td>0.011109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\n",
       "forestreg train  0.348015  0.354293  0.351649  0.364987  0.350212  0.353831   \n",
       "forestreg val    0.858008  0.847944  0.852273  0.844156  0.828591  0.846194   \n",
       "\n",
       "                      std  \n",
       "forestreg train  0.006640  \n",
       "forestreg val    0.011109  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forestreg_model = RandomForestRegressor()\n",
    "forestreg_errors = cross_val_rmse(forestreg_model, train_X, train_y, cv=5, random_state=42, model_name='forestreg')\n",
    "display(forestreg_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYF17CVpjnTm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold 0</th>\n",
       "      <th>fold 1</th>\n",
       "      <th>fold 2</th>\n",
       "      <th>fold 3</th>\n",
       "      <th>fold 4</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>linsvr train</th>\n",
       "      <td>0.906312</td>\n",
       "      <td>0.911642</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>0.902954</td>\n",
       "      <td>0.907608</td>\n",
       "      <td>0.906403</td>\n",
       "      <td>0.003508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linsvr val</th>\n",
       "      <td>0.908033</td>\n",
       "      <td>0.893220</td>\n",
       "      <td>0.923385</td>\n",
       "      <td>0.920661</td>\n",
       "      <td>0.903200</td>\n",
       "      <td>0.909700</td>\n",
       "      <td>0.012490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\n",
       "linsvr train  0.906312  0.911642  0.903500  0.902954  0.907608  0.906403   \n",
       "linsvr val    0.908033  0.893220  0.923385  0.920661  0.903200  0.909700   \n",
       "\n",
       "                   std  \n",
       "linsvr train  0.003508  \n",
       "linsvr val    0.012490  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "linsvr_model = LinearSVR(max_iter=100000)\n",
    "linsvr_errors = cross_val_rmse(linsvr_model, train_X, train_y, cv=5, random_state=42, model_name='linsvr')\n",
    "display(linsvr_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0m_pQSHJj23c",
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========] - 0s 18us/step - loss: 0.5967\n",
      "Epoch 70/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5946\n",
      "Epoch 71/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5968\n",
      "Epoch 72/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5949\n",
      "Epoch 73/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5951\n",
      "Epoch 74/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5928\n",
      "Epoch 75/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5926\n",
      "Epoch 76/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5907\n",
      "Epoch 77/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5884\n",
      "Epoch 78/150\n",
      "8672/8672 [==============================] - 0s 21us/step - loss: 0.5914\n",
      "Epoch 79/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5874\n",
      "Epoch 80/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5851\n",
      "Epoch 81/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.5856\n",
      "Epoch 82/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5843\n",
      "Epoch 83/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.5845\n",
      "Epoch 84/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5850\n",
      "Epoch 85/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5850\n",
      "Epoch 86/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5793\n",
      "Epoch 87/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5808\n",
      "Epoch 88/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5808\n",
      "Epoch 89/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5765\n",
      "Epoch 90/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5801\n",
      "Epoch 91/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5796\n",
      "Epoch 92/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5788\n",
      "Epoch 93/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5778\n",
      "Epoch 94/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5796\n",
      "Epoch 95/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.5762\n",
      "Epoch 96/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.5752\n",
      "Epoch 97/150\n",
      "8672/8672 [==============================] - 0s 23us/step - loss: 0.5735\n",
      "Epoch 98/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5718\n",
      "Epoch 99/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5757\n",
      "Epoch 100/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5746\n",
      "Epoch 101/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.5731\n",
      "Epoch 102/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5715\n",
      "Epoch 103/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5725\n",
      "Epoch 104/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5721\n",
      "Epoch 105/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5699\n",
      "Epoch 106/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5705\n",
      "Epoch 107/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5684\n",
      "Epoch 108/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5668\n",
      "Epoch 109/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5660\n",
      "Epoch 110/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5660\n",
      "Epoch 111/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5679\n",
      "Epoch 112/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5670\n",
      "Epoch 113/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5662\n",
      "Epoch 114/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5647\n",
      "Epoch 115/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.5668\n",
      "Epoch 116/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5642\n",
      "Epoch 117/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5634\n",
      "Epoch 118/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5640\n",
      "Epoch 119/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5628\n",
      "Epoch 120/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.5620\n",
      "Epoch 121/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5637\n",
      "Epoch 122/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5638\n",
      "Epoch 123/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5615\n",
      "Epoch 124/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5615\n",
      "Epoch 125/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5617\n",
      "Epoch 126/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5613\n",
      "Epoch 127/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5611\n",
      "Epoch 128/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5610\n",
      "Epoch 129/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5588\n",
      "Epoch 130/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5583\n",
      "Epoch 131/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5585\n",
      "Epoch 132/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5589\n",
      "Epoch 133/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.5569\n",
      "Epoch 134/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5583\n",
      "Epoch 135/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5556\n",
      "Epoch 136/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5553\n",
      "Epoch 137/150\n",
      "8672/8672 [==============================] - 0s 21us/step - loss: 0.5555\n",
      "Epoch 138/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5564\n",
      "Epoch 139/150\n",
      "8672/8672 [==============================] - 0s 23us/step - loss: 0.5551\n",
      "Epoch 140/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5554\n",
      "Epoch 141/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5561\n",
      "Epoch 142/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5523\n",
      "Epoch 143/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5531\n",
      "Epoch 144/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5521\n",
      "Epoch 145/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.5560\n",
      "Epoch 146/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5530\n",
      "Epoch 147/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5499\n",
      "Epoch 148/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5498\n",
      "Epoch 149/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5484\n",
      "Epoch 150/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5519\n",
      "Epoch 1/150\n",
      "8672/8672 [==============================] - 0s 31us/step - loss: 2.0295\n",
      "Epoch 2/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.8645\n",
      "Epoch 3/150\n",
      "8672/8672 [==============================] - 0s 21us/step - loss: 0.8005\n",
      "Epoch 4/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.7709\n",
      "Epoch 5/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.7543\n",
      "Epoch 6/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.7385\n",
      "Epoch 7/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.7248\n",
      "Epoch 8/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.7144\n",
      "Epoch 9/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.7050\n",
      "Epoch 10/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6990\n",
      "Epoch 11/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.6916\n",
      "Epoch 12/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.6856\n",
      "Epoch 13/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6817\n",
      "Epoch 14/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6738\n",
      "Epoch 15/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6711\n",
      "Epoch 16/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.6662\n",
      "Epoch 17/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6656\n",
      "Epoch 18/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6618\n",
      "Epoch 19/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6564\n",
      "Epoch 20/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6515\n",
      "Epoch 21/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.6500\n",
      "Epoch 22/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6455\n",
      "Epoch 23/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6425\n",
      "Epoch 24/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.6388\n",
      "Epoch 25/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6386\n",
      "Epoch 26/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6338\n",
      "Epoch 27/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6343\n",
      "Epoch 28/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6313\n",
      "Epoch 29/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6308\n",
      "Epoch 30/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6268\n",
      "Epoch 31/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6267\n",
      "Epoch 32/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6234\n",
      "Epoch 33/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6197\n",
      "Epoch 34/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.6217\n",
      "Epoch 35/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6166\n",
      "Epoch 36/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6178\n",
      "Epoch 37/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6131\n",
      "Epoch 38/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6144\n",
      "Epoch 39/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6139\n",
      "Epoch 40/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.6126\n",
      "Epoch 41/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6087\n",
      "Epoch 42/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.6081\n",
      "Epoch 43/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6080\n",
      "Epoch 44/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6037\n",
      "Epoch 45/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6030\n",
      "Epoch 46/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6021\n",
      "Epoch 47/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6021\n",
      "Epoch 48/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5979\n",
      "Epoch 49/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.6032\n",
      "Epoch 50/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5986\n",
      "Epoch 51/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5946\n",
      "Epoch 52/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5969\n",
      "Epoch 53/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5950\n",
      "Epoch 54/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5930\n",
      "Epoch 55/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5913\n",
      "Epoch 56/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5937\n",
      "Epoch 57/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5916\n",
      "Epoch 58/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5913\n",
      "Epoch 59/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5919\n",
      "Epoch 60/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5904\n",
      "Epoch 61/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5863\n",
      "Epoch 62/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5885\n",
      "Epoch 63/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5883\n",
      "Epoch 64/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5840\n",
      "Epoch 65/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5836\n",
      "Epoch 66/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5849\n",
      "Epoch 67/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5804\n",
      "Epoch 68/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5804\n",
      "Epoch 69/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5830\n",
      "Epoch 70/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5823\n",
      "Epoch 71/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5848\n",
      "Epoch 72/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5784\n",
      "Epoch 73/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5780\n",
      "Epoch 74/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5777\n",
      "Epoch 75/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5783\n",
      "Epoch 76/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5776\n",
      "Epoch 77/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5781\n",
      "Epoch 78/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5764\n",
      "Epoch 79/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5754\n",
      "Epoch 80/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5743\n",
      "Epoch 81/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5768\n",
      "Epoch 82/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5703\n",
      "Epoch 83/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5726\n",
      "Epoch 84/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5730\n",
      "Epoch 85/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5736\n",
      "Epoch 86/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5725\n",
      "Epoch 87/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5711\n",
      "Epoch 88/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5702\n",
      "Epoch 89/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5688\n",
      "Epoch 90/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5707\n",
      "Epoch 91/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5712\n",
      "Epoch 92/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5684\n",
      "Epoch 93/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5666\n",
      "Epoch 94/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5656\n",
      "Epoch 95/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5677\n",
      "Epoch 96/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5669\n",
      "Epoch 97/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5696\n",
      "Epoch 98/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5649\n",
      "Epoch 99/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5673\n",
      "Epoch 100/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5670\n",
      "Epoch 101/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5644\n",
      "Epoch 102/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5629\n",
      "Epoch 103/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5653\n",
      "Epoch 104/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5621\n",
      "Epoch 105/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5636\n",
      "Epoch 106/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5631\n",
      "Epoch 107/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5607\n",
      "Epoch 108/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5613\n",
      "Epoch 109/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5616\n",
      "Epoch 110/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5604\n",
      "Epoch 111/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5611\n",
      "Epoch 112/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5587\n",
      "Epoch 113/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5608\n",
      "Epoch 114/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5566\n",
      "Epoch 115/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5592\n",
      "Epoch 116/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5554\n",
      "Epoch 117/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.5595\n",
      "Epoch 118/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5579\n",
      "Epoch 119/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5584\n",
      "Epoch 120/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5571\n",
      "Epoch 121/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5566\n",
      "Epoch 122/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5571\n",
      "Epoch 123/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5527\n",
      "Epoch 124/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5561\n",
      "Epoch 125/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5556\n",
      "Epoch 126/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5517\n",
      "Epoch 127/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5542\n",
      "Epoch 128/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5540\n",
      "Epoch 129/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5519\n",
      "Epoch 130/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5534\n",
      "Epoch 131/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5535\n",
      "Epoch 132/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5518\n",
      "Epoch 133/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5506\n",
      "Epoch 134/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5547\n",
      "Epoch 135/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5526\n",
      "Epoch 136/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5509\n",
      "Epoch 137/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5483\n",
      "Epoch 138/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5507\n",
      "Epoch 139/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5503\n",
      "Epoch 140/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5516\n",
      "Epoch 141/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5507\n",
      "Epoch 142/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5496\n",
      "Epoch 143/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5524\n",
      "Epoch 144/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5481\n",
      "Epoch 145/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5493\n",
      "Epoch 146/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5477\n",
      "Epoch 147/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5511\n",
      "Epoch 148/150\n",
      "8672/8672 [==============================] - 0s 14us/step - loss: 0.5453\n",
      "Epoch 149/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5471\n",
      "Epoch 150/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5497\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold 0</th>\n",
       "      <th>fold 1</th>\n",
       "      <th>fold 2</th>\n",
       "      <th>fold 3</th>\n",
       "      <th>fold 4</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>neuralnet train</th>\n",
       "      <td>0.726723</td>\n",
       "      <td>0.730062</td>\n",
       "      <td>0.737216</td>\n",
       "      <td>0.727451</td>\n",
       "      <td>0.728118</td>\n",
       "      <td>0.729914</td>\n",
       "      <td>0.004266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuralnet val</th>\n",
       "      <td>0.819689</td>\n",
       "      <td>0.811171</td>\n",
       "      <td>0.827034</td>\n",
       "      <td>0.812550</td>\n",
       "      <td>0.810170</td>\n",
       "      <td>0.816123</td>\n",
       "      <td>0.007150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\n",
       "neuralnet train  0.726723  0.730062  0.737216  0.727451  0.728118  0.729914   \n",
       "neuralnet val    0.819689  0.811171  0.827034  0.812550  0.810170  0.816123   \n",
       "\n",
       "                      std  \n",
       "neuralnet train  0.004266  \n",
       "neuralnet val    0.007150  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "def create_neuralnet_model(input_shape):\n",
    "    def create_model():\n",
    "        model = Sequential([\n",
    "                            Dense(32, input_shape=input_shape, activation='relu'),\n",
    "                            Dense(1)\n",
    "                ])\n",
    "        model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "        return model\n",
    "\n",
    "    # wrap the neural network model to be used by scikit-learn\n",
    "    neuralnet_model = KerasRegressor(create_model, epochs=150)\n",
    "    return neuralnet_model\n",
    "\n",
    "neuralnet_model = create_neuralnet_model(train_X.shape[1:])\n",
    "neuralnet_errors = cross_val_rmse(neuralnet_model, train_X, train_y, cv=5, random_state=42, model_name='neuralnet')\n",
    "display(neuralnet_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SkPo0DwQu_f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold 0</th>\n",
       "      <th>fold 1</th>\n",
       "      <th>fold 2</th>\n",
       "      <th>fold 3</th>\n",
       "      <th>fold 4</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline train</th>\n",
       "      <td>1.123358</td>\n",
       "      <td>1.121115</td>\n",
       "      <td>1.114294</td>\n",
       "      <td>1.119928</td>\n",
       "      <td>1.122722</td>\n",
       "      <td>1.120283</td>\n",
       "      <td>0.003609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline val</th>\n",
       "      <td>1.107938</td>\n",
       "      <td>1.116999</td>\n",
       "      <td>1.143970</td>\n",
       "      <td>1.121741</td>\n",
       "      <td>1.110553</td>\n",
       "      <td>1.120240</td>\n",
       "      <td>0.014327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linreg train</th>\n",
       "      <td>0.893267</td>\n",
       "      <td>0.896926</td>\n",
       "      <td>0.889980</td>\n",
       "      <td>0.890549</td>\n",
       "      <td>0.894051</td>\n",
       "      <td>0.892955</td>\n",
       "      <td>0.002815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linreg val</th>\n",
       "      <td>0.897161</td>\n",
       "      <td>0.881527</td>\n",
       "      <td>0.910644</td>\n",
       "      <td>0.908679</td>\n",
       "      <td>0.894478</td>\n",
       "      <td>0.898498</td>\n",
       "      <td>0.011802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forestreg train</th>\n",
       "      <td>0.348015</td>\n",
       "      <td>0.354293</td>\n",
       "      <td>0.351649</td>\n",
       "      <td>0.364987</td>\n",
       "      <td>0.350212</td>\n",
       "      <td>0.353831</td>\n",
       "      <td>0.006640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forestreg val</th>\n",
       "      <td>0.858008</td>\n",
       "      <td>0.847944</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.828591</td>\n",
       "      <td>0.846194</td>\n",
       "      <td>0.011109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linsvr train</th>\n",
       "      <td>0.906312</td>\n",
       "      <td>0.911642</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>0.902954</td>\n",
       "      <td>0.907608</td>\n",
       "      <td>0.906403</td>\n",
       "      <td>0.003508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linsvr val</th>\n",
       "      <td>0.908033</td>\n",
       "      <td>0.893220</td>\n",
       "      <td>0.923385</td>\n",
       "      <td>0.920661</td>\n",
       "      <td>0.903200</td>\n",
       "      <td>0.909700</td>\n",
       "      <td>0.012490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuralnet train</th>\n",
       "      <td>0.726723</td>\n",
       "      <td>0.730062</td>\n",
       "      <td>0.737216</td>\n",
       "      <td>0.727451</td>\n",
       "      <td>0.728118</td>\n",
       "      <td>0.729914</td>\n",
       "      <td>0.004266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuralnet val</th>\n",
       "      <td>0.819689</td>\n",
       "      <td>0.811171</td>\n",
       "      <td>0.827034</td>\n",
       "      <td>0.812550</td>\n",
       "      <td>0.810170</td>\n",
       "      <td>0.816123</td>\n",
       "      <td>0.007150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\n",
       "baseline train   1.123358  1.121115  1.114294  1.119928  1.122722  1.120283   \n",
       "baseline val     1.107938  1.116999  1.143970  1.121741  1.110553  1.120240   \n",
       "linreg train     0.893267  0.896926  0.889980  0.890549  0.894051  0.892955   \n",
       "linreg val       0.897161  0.881527  0.910644  0.908679  0.894478  0.898498   \n",
       "forestreg train  0.348015  0.354293  0.351649  0.364987  0.350212  0.353831   \n",
       "forestreg val    0.858008  0.847944  0.852273  0.844156  0.828591  0.846194   \n",
       "linsvr train     0.906312  0.911642  0.903500  0.902954  0.907608  0.906403   \n",
       "linsvr val       0.908033  0.893220  0.923385  0.920661  0.903200  0.909700   \n",
       "neuralnet train  0.726723  0.730062  0.737216  0.727451  0.728118  0.729914   \n",
       "neuralnet val    0.819689  0.811171  0.827034  0.812550  0.810170  0.816123   \n",
       "\n",
       "                      std  \n",
       "baseline train   0.003609  \n",
       "baseline val     0.014327  \n",
       "linreg train     0.002815  \n",
       "linreg val       0.011802  \n",
       "forestreg train  0.006640  \n",
       "forestreg val    0.011109  \n",
       "linsvr train     0.003508  \n",
       "linsvr val       0.012490  \n",
       "neuralnet train  0.004266  \n",
       "neuralnet val    0.007150  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_tr_model_errors = pd.concat([baseline_errors, linreg_errors, forestreg_errors, linsvr_errors, neuralnet_errors])\n",
    "power_tr_model_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_models(train_X, train_y):\n",
    "    baseline_errors = cross_val_rmse(baseline_model, train_X, train_y, cv=5, model_name='baseline')\n",
    "    linreg_errors = cross_val_rmse(linreg_model, train_X, train_y, cv=5, model_name='linreg')\n",
    "    forestreg_errors = cross_val_rmse(forestreg_model, train_X, train_y, cv=5, model_name='forestreg')\n",
    "    linsvr_errors = cross_val_rmse(linsvr_model, train_X, train_y, cv=5, model_name='linsvr')\n",
    "\n",
    "    neuralnet_model = create_neuralnet_model(train_X.shape[1:])\n",
    "    neuralnet_errors = cross_val_rmse(neuralnet_model, train_X, train_y, cv=5, model_name='neuralnet')\n",
    "    \n",
    "    return pd.concat([baseline_errors, linreg_errors, forestreg_errors, linsvr_errors, neuralnet_errors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_kNtXOirf8Q"
   },
   "outputs": [],
   "source": [
    "train_X, train_y = separate_features_targets(training_set)\n",
    "\n",
    "# preprocess training features (standardization)\n",
    "feature_preprocessor_std = FeaturePreprocessor(add_combinations=True, std_scale_num=True, onehot_type=True)\n",
    "train_X_std = feature_preprocessor_std.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "j_pXrqpCv6ei",
    "outputId": "c4975eca-0bf3-4b51-e1c5-dd83c13821fd",
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========] - 0s 15us/step - loss: 0.6259\n",
      "Epoch 70/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6213\n",
      "Epoch 71/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6305\n",
      "Epoch 72/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6203\n",
      "Epoch 73/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6452\n",
      "Epoch 74/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6322\n",
      "Epoch 75/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6211\n",
      "Epoch 76/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6179\n",
      "Epoch 77/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6186\n",
      "Epoch 78/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6161\n",
      "Epoch 79/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6197\n",
      "Epoch 80/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6181\n",
      "Epoch 81/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6190\n",
      "Epoch 82/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.6210\n",
      "Epoch 83/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6148\n",
      "Epoch 84/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6123\n",
      "Epoch 85/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6162\n",
      "Epoch 86/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6119\n",
      "Epoch 87/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6174\n",
      "Epoch 88/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6131\n",
      "Epoch 89/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.6139\n",
      "Epoch 90/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6060\n",
      "Epoch 91/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6077\n",
      "Epoch 92/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6121\n",
      "Epoch 93/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6080\n",
      "Epoch 94/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6048\n",
      "Epoch 95/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6062\n",
      "Epoch 96/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6029\n",
      "Epoch 97/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6069\n",
      "Epoch 98/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6093\n",
      "Epoch 99/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6050\n",
      "Epoch 100/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6068\n",
      "Epoch 101/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6057\n",
      "Epoch 102/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6186\n",
      "Epoch 103/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.6200\n",
      "Epoch 104/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6049\n",
      "Epoch 105/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5993\n",
      "Epoch 106/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5968\n",
      "Epoch 107/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5976\n",
      "Epoch 108/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6000\n",
      "Epoch 109/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6010\n",
      "Epoch 110/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.5978\n",
      "Epoch 111/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5972\n",
      "Epoch 112/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5943\n",
      "Epoch 113/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5960\n",
      "Epoch 114/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5951\n",
      "Epoch 115/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5972\n",
      "Epoch 116/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5933\n",
      "Epoch 117/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5954\n",
      "Epoch 118/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5960\n",
      "Epoch 119/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6095\n",
      "Epoch 120/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5982\n",
      "Epoch 121/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5918\n",
      "Epoch 122/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5937\n",
      "Epoch 123/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6033\n",
      "Epoch 124/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6139\n",
      "Epoch 125/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5923\n",
      "Epoch 126/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5923\n",
      "Epoch 127/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5909\n",
      "Epoch 128/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5927\n",
      "Epoch 129/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5888\n",
      "Epoch 130/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5882\n",
      "Epoch 131/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5932\n",
      "Epoch 132/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5885\n",
      "Epoch 133/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5889\n",
      "Epoch 134/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5871\n",
      "Epoch 135/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5870\n",
      "Epoch 136/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5871\n",
      "Epoch 137/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5854\n",
      "Epoch 138/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5918\n",
      "Epoch 139/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5883\n",
      "Epoch 140/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5840\n",
      "Epoch 141/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5853\n",
      "Epoch 142/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5832\n",
      "Epoch 143/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5877\n",
      "Epoch 144/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5847\n",
      "Epoch 145/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5845\n",
      "Epoch 146/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5880\n",
      "Epoch 147/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5871\n",
      "Epoch 148/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5828\n",
      "Epoch 149/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5932\n",
      "Epoch 150/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5928\n",
      "Epoch 1/150\n",
      "8672/8672 [==============================] - 0s 37us/step - loss: 2.4493\n",
      "Epoch 2/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.9456\n",
      "Epoch 3/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.8678\n",
      "Epoch 4/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.8435\n",
      "Epoch 5/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.8095\n",
      "Epoch 6/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.7943\n",
      "Epoch 7/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.8073\n",
      "Epoch 8/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.7864\n",
      "Epoch 9/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.7657\n",
      "Epoch 10/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.7673\n",
      "Epoch 11/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.7597\n",
      "Epoch 12/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.7477\n",
      "Epoch 13/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.7467\n",
      "Epoch 14/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.8071\n",
      "Epoch 15/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.7421\n",
      "Epoch 16/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.7338\n",
      "Epoch 17/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.7289\n",
      "Epoch 18/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.7188\n",
      "Epoch 19/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.7397\n",
      "Epoch 20/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.7293\n",
      "Epoch 21/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.7074\n",
      "Epoch 22/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.7324\n",
      "Epoch 23/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.7006\n",
      "Epoch 24/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.7049\n",
      "Epoch 25/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.7342\n",
      "Epoch 26/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6950\n",
      "Epoch 27/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6870\n",
      "Epoch 28/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6923\n",
      "Epoch 29/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.7010\n",
      "Epoch 30/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6855\n",
      "Epoch 31/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6880\n",
      "Epoch 32/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6975\n",
      "Epoch 33/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6718\n",
      "Epoch 34/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6836\n",
      "Epoch 35/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6802\n",
      "Epoch 36/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6729\n",
      "Epoch 37/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6744\n",
      "Epoch 38/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6654\n",
      "Epoch 39/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6641\n",
      "Epoch 40/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6678\n",
      "Epoch 41/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6736\n",
      "Epoch 42/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6877\n",
      "Epoch 43/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6900\n",
      "Epoch 44/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6588\n",
      "Epoch 45/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6590\n",
      "Epoch 46/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6579\n",
      "Epoch 47/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6659\n",
      "Epoch 48/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6628\n",
      "Epoch 49/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6792\n",
      "Epoch 50/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6516\n",
      "Epoch 51/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6517\n",
      "Epoch 52/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6496\n",
      "Epoch 53/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6512\n",
      "Epoch 54/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6501\n",
      "Epoch 55/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6664\n",
      "Epoch 56/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6487\n",
      "Epoch 57/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6542\n",
      "Epoch 58/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6413\n",
      "Epoch 59/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6545\n",
      "Epoch 60/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6710\n",
      "Epoch 61/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6422\n",
      "Epoch 62/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6649\n",
      "Epoch 63/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.7154\n",
      "Epoch 64/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6514\n",
      "Epoch 65/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6435\n",
      "Epoch 66/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6405\n",
      "Epoch 67/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6448\n",
      "Epoch 68/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.7018\n",
      "Epoch 69/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6391\n",
      "Epoch 70/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6416\n",
      "Epoch 71/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6513\n",
      "Epoch 72/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6368\n",
      "Epoch 73/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6384\n",
      "Epoch 74/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6302\n",
      "Epoch 75/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6306\n",
      "Epoch 76/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6315\n",
      "Epoch 77/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6312\n",
      "Epoch 78/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6329\n",
      "Epoch 79/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6290\n",
      "Epoch 80/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6272\n",
      "Epoch 81/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6269\n",
      "Epoch 82/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6256\n",
      "Epoch 83/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6221\n",
      "Epoch 84/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6281\n",
      "Epoch 85/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6292\n",
      "Epoch 86/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6249\n",
      "Epoch 87/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6291\n",
      "Epoch 88/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6196\n",
      "Epoch 89/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6262\n",
      "Epoch 90/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6252\n",
      "Epoch 91/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6324\n",
      "Epoch 92/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.6260\n",
      "Epoch 93/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.6257\n",
      "Epoch 94/150\n",
      "8672/8672 [==============================] - 0s 23us/step - loss: 0.6235\n",
      "Epoch 95/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.6320\n",
      "Epoch 96/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6166\n",
      "Epoch 97/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6204\n",
      "Epoch 98/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6241\n",
      "Epoch 99/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6181\n",
      "Epoch 100/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6468\n",
      "Epoch 101/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6134\n",
      "Epoch 102/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6162\n",
      "Epoch 103/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6120\n",
      "Epoch 104/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6201\n",
      "Epoch 105/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6312\n",
      "Epoch 106/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6150\n",
      "Epoch 107/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6288\n",
      "Epoch 108/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6152\n",
      "Epoch 109/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6198\n",
      "Epoch 110/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.7094\n",
      "Epoch 111/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6384\n",
      "Epoch 112/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6410\n",
      "Epoch 113/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6406\n",
      "Epoch 114/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6239\n",
      "Epoch 115/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6299\n",
      "Epoch 116/150\n",
      "8672/8672 [==============================] - 0s 22us/step - loss: 0.6075\n",
      "Epoch 117/150\n",
      "8672/8672 [==============================] - 0s 21us/step - loss: 0.6096\n",
      "Epoch 118/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.6062\n",
      "Epoch 119/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.6086\n",
      "Epoch 120/150\n",
      "8672/8672 [==============================] - 0s 22us/step - loss: 0.6173\n",
      "Epoch 121/150\n",
      "8672/8672 [==============================] - 0s 22us/step - loss: 0.6080\n",
      "Epoch 122/150\n",
      "8672/8672 [==============================] - 0s 22us/step - loss: 0.6119\n",
      "Epoch 123/150\n",
      "8672/8672 [==============================] - 0s 21us/step - loss: 0.5996\n",
      "Epoch 124/150\n",
      "8672/8672 [==============================] - 0s 26us/step - loss: 0.6056\n",
      "Epoch 125/150\n",
      "8672/8672 [==============================] - 0s 26us/step - loss: 0.6037\n",
      "Epoch 126/150\n",
      "8672/8672 [==============================] - 0s 23us/step - loss: 0.6013\n",
      "Epoch 127/150\n",
      "8672/8672 [==============================] - 0s 21us/step - loss: 0.5985\n",
      "Epoch 128/150\n",
      "8672/8672 [==============================] - 0s 22us/step - loss: 0.6045\n",
      "Epoch 129/150\n",
      "8672/8672 [==============================] - 0s 22us/step - loss: 0.5983\n",
      "Epoch 130/150\n",
      "8672/8672 [==============================] - 0s 22us/step - loss: 0.6430\n",
      "Epoch 131/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.6004\n",
      "Epoch 132/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.6047\n",
      "Epoch 133/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5966\n",
      "Epoch 134/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.5986\n",
      "Epoch 135/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6149\n",
      "Epoch 136/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6079\n",
      "Epoch 137/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6080\n",
      "Epoch 138/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6041\n",
      "Epoch 139/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5950\n",
      "Epoch 140/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5971\n",
      "Epoch 141/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5966\n",
      "Epoch 142/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6009\n",
      "Epoch 143/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6106\n",
      "Epoch 144/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5912\n",
      "Epoch 145/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5922\n",
      "Epoch 146/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5966\n",
      "Epoch 147/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5933\n",
      "Epoch 148/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.6019\n",
      "Epoch 149/150\n",
      "8672/8672 [==============================] - 0s 15us/step - loss: 0.5894\n",
      "Epoch 150/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6097\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold 0</th>\n",
       "      <th>fold 1</th>\n",
       "      <th>fold 2</th>\n",
       "      <th>fold 3</th>\n",
       "      <th>fold 4</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline train</th>\n",
       "      <td>1.123358</td>\n",
       "      <td>1.121115</td>\n",
       "      <td>1.114294</td>\n",
       "      <td>1.119928</td>\n",
       "      <td>1.122722</td>\n",
       "      <td>1.120283</td>\n",
       "      <td>0.003609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline val</th>\n",
       "      <td>1.107938</td>\n",
       "      <td>1.116999</td>\n",
       "      <td>1.143970</td>\n",
       "      <td>1.121741</td>\n",
       "      <td>1.110553</td>\n",
       "      <td>1.120240</td>\n",
       "      <td>0.014327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linreg train</th>\n",
       "      <td>0.913938</td>\n",
       "      <td>0.915948</td>\n",
       "      <td>0.906405</td>\n",
       "      <td>0.909648</td>\n",
       "      <td>0.916653</td>\n",
       "      <td>0.912518</td>\n",
       "      <td>0.004373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linreg val</th>\n",
       "      <td>0.909653</td>\n",
       "      <td>0.900677</td>\n",
       "      <td>0.939456</td>\n",
       "      <td>0.929410</td>\n",
       "      <td>0.911433</td>\n",
       "      <td>0.918126</td>\n",
       "      <td>0.015838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forestreg train</th>\n",
       "      <td>0.352452</td>\n",
       "      <td>0.357309</td>\n",
       "      <td>0.352901</td>\n",
       "      <td>0.359270</td>\n",
       "      <td>0.356300</td>\n",
       "      <td>0.355646</td>\n",
       "      <td>0.002918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forestreg val</th>\n",
       "      <td>0.855975</td>\n",
       "      <td>0.828011</td>\n",
       "      <td>0.852394</td>\n",
       "      <td>0.868915</td>\n",
       "      <td>0.830429</td>\n",
       "      <td>0.847145</td>\n",
       "      <td>0.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linsvr train</th>\n",
       "      <td>0.931163</td>\n",
       "      <td>0.933242</td>\n",
       "      <td>0.923078</td>\n",
       "      <td>0.925309</td>\n",
       "      <td>0.929819</td>\n",
       "      <td>0.928522</td>\n",
       "      <td>0.004210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linsvr val</th>\n",
       "      <td>0.922091</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.954623</td>\n",
       "      <td>0.948079</td>\n",
       "      <td>0.918281</td>\n",
       "      <td>0.932615</td>\n",
       "      <td>0.017312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuralnet train</th>\n",
       "      <td>0.754513</td>\n",
       "      <td>0.765305</td>\n",
       "      <td>0.760587</td>\n",
       "      <td>0.761416</td>\n",
       "      <td>0.888814</td>\n",
       "      <td>0.786127</td>\n",
       "      <td>0.057534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuralnet val</th>\n",
       "      <td>0.847024</td>\n",
       "      <td>0.815533</td>\n",
       "      <td>0.854683</td>\n",
       "      <td>0.849999</td>\n",
       "      <td>0.940134</td>\n",
       "      <td>0.861475</td>\n",
       "      <td>0.046596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\n",
       "baseline train   1.123358  1.121115  1.114294  1.119928  1.122722  1.120283   \n",
       "baseline val     1.107938  1.116999  1.143970  1.121741  1.110553  1.120240   \n",
       "linreg train     0.913938  0.915948  0.906405  0.909648  0.916653  0.912518   \n",
       "linreg val       0.909653  0.900677  0.939456  0.929410  0.911433  0.918126   \n",
       "forestreg train  0.352452  0.357309  0.352901  0.359270  0.356300  0.355646   \n",
       "forestreg val    0.855975  0.828011  0.852394  0.868915  0.830429  0.847145   \n",
       "linsvr train     0.931163  0.933242  0.923078  0.925309  0.929819  0.928522   \n",
       "linsvr val       0.922091  0.920000  0.954623  0.948079  0.918281  0.932615   \n",
       "neuralnet train  0.754513  0.765305  0.760587  0.761416  0.888814  0.786127   \n",
       "neuralnet val    0.847024  0.815533  0.854683  0.849999  0.940134  0.861475   \n",
       "\n",
       "                      std  \n",
       "baseline train   0.003609  \n",
       "baseline val     0.014327  \n",
       "linreg train     0.004373  \n",
       "linreg val       0.015838  \n",
       "forestreg train  0.002918  \n",
       "forestreg val    0.017500  \n",
       "linsvr train     0.004210  \n",
       "linsvr val       0.017312  \n",
       "neuralnet train  0.057534  \n",
       "neuralnet val    0.046596  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_errors = evaluate_models(train_X_std, train_y)\n",
    "std_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "UhAzmaTKoYYp",
    "outputId": "891c79ed-9fa3-4109-fe84-8272f84d087d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X, train_y = separate_features_targets(training_set)\n",
    "\n",
    "train_X = feature_preprocessor.fit_transform(train_X)\n",
    "forestreg_model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4ruWR7baoznC",
    "outputId": "3080df9f-ec1f-4263-bd2d-cd6e1a304baa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>year</td>\n",
       "      <td>0.138135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ratingCount</td>\n",
       "      <td>0.126447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reviewsPerRating</td>\n",
       "      <td>0.117280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>duration</td>\n",
       "      <td>0.095743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nrOfWins</td>\n",
       "      <td>0.078460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_video.movie</td>\n",
       "      <td>0.076736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nrOfUserReviews</td>\n",
       "      <td>0.043138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nrOfPhotos</td>\n",
       "      <td>0.038654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nrOfNewsArticles</td>\n",
       "      <td>0.038006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>winsPerNomination</td>\n",
       "      <td>0.025696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Documentary</td>\n",
       "      <td>0.020118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Drama</td>\n",
       "      <td>0.016692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>totalNominations</td>\n",
       "      <td>0.015943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nrOfGenre</td>\n",
       "      <td>0.014991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SciFi</td>\n",
       "      <td>0.014905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Horror</td>\n",
       "      <td>0.013746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Comedy</td>\n",
       "      <td>0.012995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Animation</td>\n",
       "      <td>0.010112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>type_video.tv</td>\n",
       "      <td>0.009749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Family</td>\n",
       "      <td>0.008361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>nrOfNominations</td>\n",
       "      <td>0.008039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>News</td>\n",
       "      <td>0.007365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Action</td>\n",
       "      <td>0.007108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RealityTV</td>\n",
       "      <td>0.007052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Fantasy</td>\n",
       "      <td>0.006416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Romance</td>\n",
       "      <td>0.006156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Adventure</td>\n",
       "      <td>0.005302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>type_video.episode</td>\n",
       "      <td>0.004656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Crime</td>\n",
       "      <td>0.004322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Musical</td>\n",
       "      <td>0.004315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Music</td>\n",
       "      <td>0.004068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Thriller</td>\n",
       "      <td>0.003726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>TalkShow</td>\n",
       "      <td>0.003646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Short</td>\n",
       "      <td>0.002142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Mystery</td>\n",
       "      <td>0.001997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Biography</td>\n",
       "      <td>0.001616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>History</td>\n",
       "      <td>0.001397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>War</td>\n",
       "      <td>0.000992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Western</td>\n",
       "      <td>0.000972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>GameShow</td>\n",
       "      <td>0.000936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Sport</td>\n",
       "      <td>0.000885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Adult</td>\n",
       "      <td>0.000556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>type_game</td>\n",
       "      <td>0.000309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>FilmNoir</td>\n",
       "      <td>0.000123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               feature  importance\n",
       "0                 year    0.138135\n",
       "1          ratingCount    0.126447\n",
       "2     reviewsPerRating    0.117280\n",
       "3             duration    0.095743\n",
       "4             nrOfWins    0.078460\n",
       "5     type_video.movie    0.076736\n",
       "6      nrOfUserReviews    0.043138\n",
       "7           nrOfPhotos    0.038654\n",
       "8     nrOfNewsArticles    0.038006\n",
       "9    winsPerNomination    0.025696\n",
       "10         Documentary    0.020118\n",
       "11               Drama    0.016692\n",
       "12    totalNominations    0.015943\n",
       "13           nrOfGenre    0.014991\n",
       "14               SciFi    0.014905\n",
       "15              Horror    0.013746\n",
       "16              Comedy    0.012995\n",
       "17           Animation    0.010112\n",
       "18       type_video.tv    0.009749\n",
       "19              Family    0.008361\n",
       "20     nrOfNominations    0.008039\n",
       "21                News    0.007365\n",
       "22              Action    0.007108\n",
       "23           RealityTV    0.007052\n",
       "24             Fantasy    0.006416\n",
       "25             Romance    0.006156\n",
       "26           Adventure    0.005302\n",
       "27  type_video.episode    0.004656\n",
       "28               Crime    0.004322\n",
       "29             Musical    0.004315\n",
       "30               Music    0.004068\n",
       "31            Thriller    0.003726\n",
       "32            TalkShow    0.003646\n",
       "33               Short    0.002142\n",
       "34             Mystery    0.001997\n",
       "35           Biography    0.001616\n",
       "36             History    0.001397\n",
       "37                 War    0.000992\n",
       "38             Western    0.000972\n",
       "39            GameShow    0.000936\n",
       "40               Sport    0.000885\n",
       "41               Adult    0.000556\n",
       "42           type_game    0.000309\n",
       "43            FilmNoir    0.000123"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forestreg_feature_importances = pd.DataFrame({'feature': train_X.columns, 'importance': forestreg_model.feature_importances_})\n",
    "forestreg_feature_importances = forestreg_feature_importances.sort_values(by='importance', ascending=False)\n",
    "forestreg_feature_importances = forestreg_feature_importances.reset_index(drop=True)\n",
    "forestreg_feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "VijpKF9a1SiM",
    "outputId": "592441ba-d9f9-49fc-da01-c1eecf92dfc6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mystery',\n",
       " 'Biography',\n",
       " 'History',\n",
       " 'War',\n",
       " 'Western',\n",
       " 'GameShow',\n",
       " 'Sport',\n",
       " 'Adult',\n",
       " 'type_game',\n",
       " 'FilmNoir']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_important_features = list(forestreg_feature_importances.iloc[-10:]['feature'].values)\n",
    "least_important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "70cpd-6nzqpB",
    "outputId": "465b443a-fe45-4e53-9ac9-1b152c64a89a"
   },
   "outputs": [],
   "source": [
    "train_X, train_y = separate_features_targets(training_set)\n",
    "\n",
    "# preprocess training features (power transform, remove least important features)\n",
    "feature_preprocessor = FeaturePreprocessor(add_combinations=True, powertransform_num=True, onehot_type=True,\n",
    "                                           drop_features=least_important_features)\n",
    "train_X = feature_preprocessor.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "pM36l8Cg3i0v",
    "outputId": "2533f3fa-0c3b-4200-a129-b6fe95c5589f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10840 entries, 11613 to 14254\n",
      "Data columns (total 34 columns):\n",
      "ratingCount           10840 non-null float64\n",
      "duration              10840 non-null float64\n",
      "year                  10840 non-null float64\n",
      "nrOfWins              10840 non-null float64\n",
      "nrOfNominations       10840 non-null float64\n",
      "nrOfPhotos            10840 non-null float64\n",
      "nrOfNewsArticles      10840 non-null float64\n",
      "nrOfUserReviews       10840 non-null float64\n",
      "nrOfGenre             10840 non-null float64\n",
      "totalNominations      10840 non-null float64\n",
      "winsPerNomination     10840 non-null float64\n",
      "reviewsPerRating      10840 non-null float64\n",
      "type_video.episode    10840 non-null float64\n",
      "type_video.movie      10840 non-null float64\n",
      "type_video.tv         10840 non-null float64\n",
      "Action                10840 non-null int64\n",
      "Adventure             10840 non-null int64\n",
      "Animation             10840 non-null int64\n",
      "Comedy                10840 non-null int64\n",
      "Crime                 10840 non-null int64\n",
      "Documentary           10840 non-null int64\n",
      "Drama                 10840 non-null int64\n",
      "Family                10840 non-null int64\n",
      "Fantasy               10840 non-null int64\n",
      "Horror                10840 non-null int64\n",
      "Music                 10840 non-null int64\n",
      "Musical               10840 non-null int64\n",
      "News                  10840 non-null int64\n",
      "RealityTV             10840 non-null int64\n",
      "Romance               10840 non-null int64\n",
      "SciFi                 10840 non-null int64\n",
      "Short                 10840 non-null int64\n",
      "TalkShow              10840 non-null int64\n",
      "Thriller              10840 non-null int64\n",
      "dtypes: float64(15), int64(19)\n",
      "memory usage: 2.9 MB\n"
     ]
    }
   ],
   "source": [
    "train_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "K2w1K7640I42",
    "outputId": "ba9f2b98-9cb7-49ac-f88d-ca510a7dfc5c",
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========] - 0s 17us/step - loss: 0.6064\n",
      "Epoch 70/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6044\n",
      "Epoch 71/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6055\n",
      "Epoch 72/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6053\n",
      "Epoch 73/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6023\n",
      "Epoch 74/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6045\n",
      "Epoch 75/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6046\n",
      "Epoch 76/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6019\n",
      "Epoch 77/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6012\n",
      "Epoch 78/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6046\n",
      "Epoch 79/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6014\n",
      "Epoch 80/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6012\n",
      "Epoch 81/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5988\n",
      "Epoch 82/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5998\n",
      "Epoch 83/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5982\n",
      "Epoch 84/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5959\n",
      "Epoch 85/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5960\n",
      "Epoch 86/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5998\n",
      "Epoch 87/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5980\n",
      "Epoch 88/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5960\n",
      "Epoch 89/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5964\n",
      "Epoch 90/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5948\n",
      "Epoch 91/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5954\n",
      "Epoch 92/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5942\n",
      "Epoch 93/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5950\n",
      "Epoch 94/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5927\n",
      "Epoch 95/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5917\n",
      "Epoch 96/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5936\n",
      "Epoch 97/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5932\n",
      "Epoch 98/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5939\n",
      "Epoch 99/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5914\n",
      "Epoch 100/150\n",
      "8672/8672 [==============================] - 0s 22us/step - loss: 0.5899\n",
      "Epoch 101/150\n",
      "8672/8672 [==============================] - 0s 23us/step - loss: 0.5902\n",
      "Epoch 102/150\n",
      "8672/8672 [==============================] - 0s 21us/step - loss: 0.5906\n",
      "Epoch 103/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.5887\n",
      "Epoch 104/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.5911\n",
      "Epoch 105/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5892\n",
      "Epoch 106/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5899\n",
      "Epoch 107/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5884\n",
      "Epoch 108/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5882\n",
      "Epoch 109/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5884\n",
      "Epoch 110/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5887\n",
      "Epoch 111/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5869\n",
      "Epoch 112/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5869\n",
      "Epoch 113/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5854\n",
      "Epoch 114/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5852\n",
      "Epoch 115/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5840\n",
      "Epoch 116/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5833\n",
      "Epoch 117/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5807\n",
      "Epoch 118/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5828\n",
      "Epoch 119/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5841\n",
      "Epoch 120/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5811\n",
      "Epoch 121/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5818\n",
      "Epoch 122/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5812\n",
      "Epoch 123/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5818\n",
      "Epoch 124/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5790\n",
      "Epoch 125/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5806\n",
      "Epoch 126/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5802\n",
      "Epoch 127/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5762\n",
      "Epoch 128/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5787\n",
      "Epoch 129/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5776\n",
      "Epoch 130/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5769\n",
      "Epoch 131/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5762\n",
      "Epoch 132/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5762\n",
      "Epoch 133/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5758\n",
      "Epoch 134/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5772\n",
      "Epoch 135/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5740\n",
      "Epoch 136/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5737\n",
      "Epoch 137/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5750\n",
      "Epoch 138/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5749\n",
      "Epoch 139/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5745\n",
      "Epoch 140/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5731\n",
      "Epoch 141/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5738\n",
      "Epoch 142/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5711\n",
      "Epoch 143/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5704\n",
      "Epoch 144/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5725\n",
      "Epoch 145/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5708\n",
      "Epoch 146/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5727\n",
      "Epoch 147/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5695\n",
      "Epoch 148/150\n",
      "8672/8672 [==============================] - 0s 21us/step - loss: 0.5709\n",
      "Epoch 149/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5677\n",
      "Epoch 150/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5691\n",
      "Epoch 1/150\n",
      "8672/8672 [==============================] - 0s 47us/step - loss: 1.8741\n",
      "Epoch 2/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.8990\n",
      "Epoch 3/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.8329\n",
      "Epoch 4/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.7964\n",
      "Epoch 5/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.7720\n",
      "Epoch 6/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.7542\n",
      "Epoch 7/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.7413\n",
      "Epoch 8/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.7313\n",
      "Epoch 9/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.7220\n",
      "Epoch 10/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.7159\n",
      "Epoch 11/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.7060\n",
      "Epoch 12/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6996\n",
      "Epoch 13/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6964\n",
      "Epoch 14/150\n",
      "8672/8672 [==============================] - 0s 22us/step - loss: 0.6879\n",
      "Epoch 15/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.6858\n",
      "Epoch 16/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.6829\n",
      "Epoch 17/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6747\n",
      "Epoch 18/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6728\n",
      "Epoch 19/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6692\n",
      "Epoch 20/150\n",
      "8672/8672 [==============================] - 0s 22us/step - loss: 0.6678\n",
      "Epoch 21/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6657\n",
      "Epoch 22/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6611\n",
      "Epoch 23/150\n",
      "8672/8672 [==============================] - 0s 21us/step - loss: 0.6593\n",
      "Epoch 24/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.6565\n",
      "Epoch 25/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.6548\n",
      "Epoch 26/150\n",
      "8672/8672 [==============================] - 0s 21us/step - loss: 0.6538\n",
      "Epoch 27/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6504\n",
      "Epoch 28/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6488\n",
      "Epoch 29/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6469\n",
      "Epoch 30/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6438\n",
      "Epoch 31/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6422\n",
      "Epoch 32/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6399\n",
      "Epoch 33/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6400\n",
      "Epoch 34/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.6390\n",
      "Epoch 35/150\n",
      "8672/8672 [==============================] - 0s 21us/step - loss: 0.6385\n",
      "Epoch 36/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.6365\n",
      "Epoch 37/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.6342\n",
      "Epoch 38/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6330\n",
      "Epoch 39/150\n",
      "8672/8672 [==============================] - 0s 22us/step - loss: 0.6326\n",
      "Epoch 40/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6306\n",
      "Epoch 41/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6296\n",
      "Epoch 42/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.6272\n",
      "Epoch 43/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6297\n",
      "Epoch 44/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6242\n",
      "Epoch 45/150\n",
      "8672/8672 [==============================] - 0s 21us/step - loss: 0.6238\n",
      "Epoch 46/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.6246\n",
      "Epoch 47/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.6239\n",
      "Epoch 48/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6228\n",
      "Epoch 49/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6221\n",
      "Epoch 50/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6189\n",
      "Epoch 51/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6200\n",
      "Epoch 52/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6189\n",
      "Epoch 53/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6175\n",
      "Epoch 54/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6177\n",
      "Epoch 55/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6145\n",
      "Epoch 56/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6138\n",
      "Epoch 57/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6182\n",
      "Epoch 58/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.6130\n",
      "Epoch 59/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6141\n",
      "Epoch 60/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6117\n",
      "Epoch 61/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6106\n",
      "Epoch 62/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6074\n",
      "Epoch 63/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6098\n",
      "Epoch 64/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6080\n",
      "Epoch 65/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6078\n",
      "Epoch 66/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6071\n",
      "Epoch 67/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.6063\n",
      "Epoch 68/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.6041\n",
      "Epoch 69/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6045\n",
      "Epoch 70/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.6043\n",
      "Epoch 71/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.6027\n",
      "Epoch 72/150\n",
      "8672/8672 [==============================] - 0s 21us/step - loss: 0.6024\n",
      "Epoch 73/150\n",
      "8672/8672 [==============================] - 0s 22us/step - loss: 0.6023\n",
      "Epoch 74/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.6041\n",
      "Epoch 75/150\n",
      "8672/8672 [==============================] - 0s 27us/step - loss: 0.6025\n",
      "Epoch 76/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5989\n",
      "Epoch 77/150\n",
      "8672/8672 [==============================] - 0s 21us/step - loss: 0.5982\n",
      "Epoch 78/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5969\n",
      "Epoch 79/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5981\n",
      "Epoch 80/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5953\n",
      "Epoch 81/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5978\n",
      "Epoch 82/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5955\n",
      "Epoch 83/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5951\n",
      "Epoch 84/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5948\n",
      "Epoch 85/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5924\n",
      "Epoch 86/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5948\n",
      "Epoch 87/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5923\n",
      "Epoch 88/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5922\n",
      "Epoch 89/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5912\n",
      "Epoch 90/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5918\n",
      "Epoch 91/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5918\n",
      "Epoch 92/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5904\n",
      "Epoch 93/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5890\n",
      "Epoch 94/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5887\n",
      "Epoch 95/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5879\n",
      "Epoch 96/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5862\n",
      "Epoch 97/150\n",
      "8672/8672 [==============================] - 0s 25us/step - loss: 0.5874\n",
      "Epoch 98/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5866\n",
      "Epoch 99/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5846\n",
      "Epoch 100/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5838\n",
      "Epoch 101/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5863\n",
      "Epoch 102/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5843\n",
      "Epoch 103/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5862\n",
      "Epoch 104/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5835\n",
      "Epoch 105/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5835\n",
      "Epoch 106/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5819\n",
      "Epoch 107/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5801\n",
      "Epoch 108/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5807\n",
      "Epoch 109/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5825\n",
      "Epoch 110/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5794\n",
      "Epoch 111/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5826\n",
      "Epoch 112/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5797\n",
      "Epoch 113/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5822\n",
      "Epoch 114/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5798\n",
      "Epoch 115/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5790\n",
      "Epoch 116/150\n",
      "8672/8672 [==============================] - 0s 22us/step - loss: 0.5786\n",
      "Epoch 117/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5785\n",
      "Epoch 118/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5768\n",
      "Epoch 119/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5767\n",
      "Epoch 120/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.5757\n",
      "Epoch 121/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5765\n",
      "Epoch 122/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.5738\n",
      "Epoch 123/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5780\n",
      "Epoch 124/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5741\n",
      "Epoch 125/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5732\n",
      "Epoch 126/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5706\n",
      "Epoch 127/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5720\n",
      "Epoch 128/150\n",
      "8672/8672 [==============================] - 0s 21us/step - loss: 0.5735\n",
      "Epoch 129/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5712\n",
      "Epoch 130/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5714\n",
      "Epoch 131/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5723\n",
      "Epoch 132/150\n",
      "8672/8672 [==============================] - 0s 16us/step - loss: 0.5725\n",
      "Epoch 133/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5714\n",
      "Epoch 134/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5706\n",
      "Epoch 135/150\n",
      "8672/8672 [==============================] - 0s 21us/step - loss: 0.5710\n",
      "Epoch 136/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5711\n",
      "Epoch 137/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5707\n",
      "Epoch 138/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5692\n",
      "Epoch 139/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5702\n",
      "Epoch 140/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5666\n",
      "Epoch 141/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5681\n",
      "Epoch 142/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5692\n",
      "Epoch 143/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5686\n",
      "Epoch 144/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5686\n",
      "Epoch 145/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5669\n",
      "Epoch 146/150\n",
      "8672/8672 [==============================] - 0s 17us/step - loss: 0.5658\n",
      "Epoch 147/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5672\n",
      "Epoch 148/150\n",
      "8672/8672 [==============================] - 0s 19us/step - loss: 0.5680\n",
      "Epoch 149/150\n",
      "8672/8672 [==============================] - 0s 20us/step - loss: 0.5634\n",
      "Epoch 150/150\n",
      "8672/8672 [==============================] - 0s 18us/step - loss: 0.5638\n"
     ]
    }
   ],
   "source": [
    "power_tr_drop_errors = evaluate_models(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "colab_type": "code",
    "id": "z2yugWWN0YRF",
    "outputId": "8e79b269-3acf-484b-afcd-ac7259654f5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardization Only\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold 0</th>\n",
       "      <th>fold 1</th>\n",
       "      <th>fold 2</th>\n",
       "      <th>fold 3</th>\n",
       "      <th>fold 4</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline train</th>\n",
       "      <td>1.123358</td>\n",
       "      <td>1.121115</td>\n",
       "      <td>1.114294</td>\n",
       "      <td>1.119928</td>\n",
       "      <td>1.122722</td>\n",
       "      <td>1.120283</td>\n",
       "      <td>0.003609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline val</th>\n",
       "      <td>1.107938</td>\n",
       "      <td>1.116999</td>\n",
       "      <td>1.143970</td>\n",
       "      <td>1.121741</td>\n",
       "      <td>1.110553</td>\n",
       "      <td>1.120240</td>\n",
       "      <td>0.014327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linreg train</th>\n",
       "      <td>0.913938</td>\n",
       "      <td>0.915948</td>\n",
       "      <td>0.906405</td>\n",
       "      <td>0.909648</td>\n",
       "      <td>0.916653</td>\n",
       "      <td>0.912518</td>\n",
       "      <td>0.004373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linreg val</th>\n",
       "      <td>0.909653</td>\n",
       "      <td>0.900677</td>\n",
       "      <td>0.939456</td>\n",
       "      <td>0.929410</td>\n",
       "      <td>0.911433</td>\n",
       "      <td>0.918126</td>\n",
       "      <td>0.015838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forestreg train</th>\n",
       "      <td>0.352452</td>\n",
       "      <td>0.357309</td>\n",
       "      <td>0.352901</td>\n",
       "      <td>0.359270</td>\n",
       "      <td>0.356300</td>\n",
       "      <td>0.355646</td>\n",
       "      <td>0.002918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forestreg val</th>\n",
       "      <td>0.855975</td>\n",
       "      <td>0.828011</td>\n",
       "      <td>0.852394</td>\n",
       "      <td>0.868915</td>\n",
       "      <td>0.830429</td>\n",
       "      <td>0.847145</td>\n",
       "      <td>0.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linsvr train</th>\n",
       "      <td>0.931163</td>\n",
       "      <td>0.933242</td>\n",
       "      <td>0.923078</td>\n",
       "      <td>0.925309</td>\n",
       "      <td>0.929819</td>\n",
       "      <td>0.928522</td>\n",
       "      <td>0.004210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linsvr val</th>\n",
       "      <td>0.922091</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.954623</td>\n",
       "      <td>0.948079</td>\n",
       "      <td>0.918281</td>\n",
       "      <td>0.932615</td>\n",
       "      <td>0.017312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuralnet train</th>\n",
       "      <td>0.754513</td>\n",
       "      <td>0.765305</td>\n",
       "      <td>0.760587</td>\n",
       "      <td>0.761416</td>\n",
       "      <td>0.888814</td>\n",
       "      <td>0.786127</td>\n",
       "      <td>0.057534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuralnet val</th>\n",
       "      <td>0.847024</td>\n",
       "      <td>0.815533</td>\n",
       "      <td>0.854683</td>\n",
       "      <td>0.849999</td>\n",
       "      <td>0.940134</td>\n",
       "      <td>0.861475</td>\n",
       "      <td>0.046596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\n",
       "baseline train   1.123358  1.121115  1.114294  1.119928  1.122722  1.120283   \n",
       "baseline val     1.107938  1.116999  1.143970  1.121741  1.110553  1.120240   \n",
       "linreg train     0.913938  0.915948  0.906405  0.909648  0.916653  0.912518   \n",
       "linreg val       0.909653  0.900677  0.939456  0.929410  0.911433  0.918126   \n",
       "forestreg train  0.352452  0.357309  0.352901  0.359270  0.356300  0.355646   \n",
       "forestreg val    0.855975  0.828011  0.852394  0.868915  0.830429  0.847145   \n",
       "linsvr train     0.931163  0.933242  0.923078  0.925309  0.929819  0.928522   \n",
       "linsvr val       0.922091  0.920000  0.954623  0.948079  0.918281  0.932615   \n",
       "neuralnet train  0.754513  0.765305  0.760587  0.761416  0.888814  0.786127   \n",
       "neuralnet val    0.847024  0.815533  0.854683  0.849999  0.940134  0.861475   \n",
       "\n",
       "                      std  \n",
       "baseline train   0.003609  \n",
       "baseline val     0.014327  \n",
       "linreg train     0.004373  \n",
       "linreg val       0.015838  \n",
       "forestreg train  0.002918  \n",
       "forestreg val    0.017500  \n",
       "linsvr train     0.004210  \n",
       "linsvr val       0.017312  \n",
       "neuralnet train  0.057534  \n",
       "neuralnet val    0.046596  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Power Transform Only\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold 0</th>\n",
       "      <th>fold 1</th>\n",
       "      <th>fold 2</th>\n",
       "      <th>fold 3</th>\n",
       "      <th>fold 4</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline train</th>\n",
       "      <td>1.123358</td>\n",
       "      <td>1.121115</td>\n",
       "      <td>1.114294</td>\n",
       "      <td>1.119928</td>\n",
       "      <td>1.122722</td>\n",
       "      <td>1.120283</td>\n",
       "      <td>0.003609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline val</th>\n",
       "      <td>1.107938</td>\n",
       "      <td>1.116999</td>\n",
       "      <td>1.143970</td>\n",
       "      <td>1.121741</td>\n",
       "      <td>1.110553</td>\n",
       "      <td>1.120240</td>\n",
       "      <td>0.014327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linreg train</th>\n",
       "      <td>0.893267</td>\n",
       "      <td>0.896926</td>\n",
       "      <td>0.889980</td>\n",
       "      <td>0.890549</td>\n",
       "      <td>0.894051</td>\n",
       "      <td>0.892955</td>\n",
       "      <td>0.002815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linreg val</th>\n",
       "      <td>0.897161</td>\n",
       "      <td>0.881527</td>\n",
       "      <td>0.910644</td>\n",
       "      <td>0.908679</td>\n",
       "      <td>0.894478</td>\n",
       "      <td>0.898498</td>\n",
       "      <td>0.011802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forestreg train</th>\n",
       "      <td>0.348015</td>\n",
       "      <td>0.354293</td>\n",
       "      <td>0.351649</td>\n",
       "      <td>0.364987</td>\n",
       "      <td>0.350212</td>\n",
       "      <td>0.353831</td>\n",
       "      <td>0.006640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forestreg val</th>\n",
       "      <td>0.858008</td>\n",
       "      <td>0.847944</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.828591</td>\n",
       "      <td>0.846194</td>\n",
       "      <td>0.011109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linsvr train</th>\n",
       "      <td>0.906312</td>\n",
       "      <td>0.911642</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>0.902954</td>\n",
       "      <td>0.907608</td>\n",
       "      <td>0.906403</td>\n",
       "      <td>0.003508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linsvr val</th>\n",
       "      <td>0.908033</td>\n",
       "      <td>0.893220</td>\n",
       "      <td>0.923385</td>\n",
       "      <td>0.920661</td>\n",
       "      <td>0.903200</td>\n",
       "      <td>0.909700</td>\n",
       "      <td>0.012490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuralnet train</th>\n",
       "      <td>0.726723</td>\n",
       "      <td>0.730062</td>\n",
       "      <td>0.737216</td>\n",
       "      <td>0.727451</td>\n",
       "      <td>0.728118</td>\n",
       "      <td>0.729914</td>\n",
       "      <td>0.004266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuralnet val</th>\n",
       "      <td>0.819689</td>\n",
       "      <td>0.811171</td>\n",
       "      <td>0.827034</td>\n",
       "      <td>0.812550</td>\n",
       "      <td>0.810170</td>\n",
       "      <td>0.816123</td>\n",
       "      <td>0.007150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\n",
       "baseline train   1.123358  1.121115  1.114294  1.119928  1.122722  1.120283   \n",
       "baseline val     1.107938  1.116999  1.143970  1.121741  1.110553  1.120240   \n",
       "linreg train     0.893267  0.896926  0.889980  0.890549  0.894051  0.892955   \n",
       "linreg val       0.897161  0.881527  0.910644  0.908679  0.894478  0.898498   \n",
       "forestreg train  0.348015  0.354293  0.351649  0.364987  0.350212  0.353831   \n",
       "forestreg val    0.858008  0.847944  0.852273  0.844156  0.828591  0.846194   \n",
       "linsvr train     0.906312  0.911642  0.903500  0.902954  0.907608  0.906403   \n",
       "linsvr val       0.908033  0.893220  0.923385  0.920661  0.903200  0.909700   \n",
       "neuralnet train  0.726723  0.730062  0.737216  0.727451  0.728118  0.729914   \n",
       "neuralnet val    0.819689  0.811171  0.827034  0.812550  0.810170  0.816123   \n",
       "\n",
       "                      std  \n",
       "baseline train   0.003609  \n",
       "baseline val     0.014327  \n",
       "linreg train     0.002815  \n",
       "linreg val       0.011802  \n",
       "forestreg train  0.006640  \n",
       "forestreg val    0.011109  \n",
       "linsvr train     0.003508  \n",
       "linsvr val       0.012490  \n",
       "neuralnet train  0.004266  \n",
       "neuralnet val    0.007150  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Power Transform and Drop 10 Least Important Features\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold 0</th>\n",
       "      <th>fold 1</th>\n",
       "      <th>fold 2</th>\n",
       "      <th>fold 3</th>\n",
       "      <th>fold 4</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline train</th>\n",
       "      <td>1.123358</td>\n",
       "      <td>1.121115</td>\n",
       "      <td>1.114294</td>\n",
       "      <td>1.119928</td>\n",
       "      <td>1.122722</td>\n",
       "      <td>1.120283</td>\n",
       "      <td>0.003609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline val</th>\n",
       "      <td>1.107938</td>\n",
       "      <td>1.116999</td>\n",
       "      <td>1.143970</td>\n",
       "      <td>1.121741</td>\n",
       "      <td>1.110553</td>\n",
       "      <td>1.120240</td>\n",
       "      <td>0.014327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linreg train</th>\n",
       "      <td>0.894511</td>\n",
       "      <td>0.898298</td>\n",
       "      <td>0.891318</td>\n",
       "      <td>0.891449</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.894220</td>\n",
       "      <td>0.002937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linreg val</th>\n",
       "      <td>0.897661</td>\n",
       "      <td>0.881696</td>\n",
       "      <td>0.910642</td>\n",
       "      <td>0.910051</td>\n",
       "      <td>0.893674</td>\n",
       "      <td>0.898745</td>\n",
       "      <td>0.012113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forestreg train</th>\n",
       "      <td>0.352870</td>\n",
       "      <td>0.361841</td>\n",
       "      <td>0.353304</td>\n",
       "      <td>0.364521</td>\n",
       "      <td>0.354510</td>\n",
       "      <td>0.357409</td>\n",
       "      <td>0.005387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forestreg val</th>\n",
       "      <td>0.844424</td>\n",
       "      <td>0.836910</td>\n",
       "      <td>0.854385</td>\n",
       "      <td>0.845759</td>\n",
       "      <td>0.823655</td>\n",
       "      <td>0.841027</td>\n",
       "      <td>0.011523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linsvr train</th>\n",
       "      <td>0.907547</td>\n",
       "      <td>0.913225</td>\n",
       "      <td>0.904705</td>\n",
       "      <td>0.903970</td>\n",
       "      <td>0.908863</td>\n",
       "      <td>0.907662</td>\n",
       "      <td>0.003701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linsvr val</th>\n",
       "      <td>0.908602</td>\n",
       "      <td>0.894419</td>\n",
       "      <td>0.923609</td>\n",
       "      <td>0.921272</td>\n",
       "      <td>0.904178</td>\n",
       "      <td>0.910416</td>\n",
       "      <td>0.012145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuralnet train</th>\n",
       "      <td>0.755310</td>\n",
       "      <td>0.748043</td>\n",
       "      <td>0.740307</td>\n",
       "      <td>0.751168</td>\n",
       "      <td>0.744720</td>\n",
       "      <td>0.747909</td>\n",
       "      <td>0.005774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuralnet val</th>\n",
       "      <td>0.824819</td>\n",
       "      <td>0.807315</td>\n",
       "      <td>0.827559</td>\n",
       "      <td>0.832562</td>\n",
       "      <td>0.826229</td>\n",
       "      <td>0.823697</td>\n",
       "      <td>0.009612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   fold 0    fold 1    fold 2    fold 3    fold 4      mean  \\\n",
       "baseline train   1.123358  1.121115  1.114294  1.119928  1.122722  1.120283   \n",
       "baseline val     1.107938  1.116999  1.143970  1.121741  1.110553  1.120240   \n",
       "linreg train     0.894511  0.898298  0.891318  0.891449  0.895522  0.894220   \n",
       "linreg val       0.897661  0.881696  0.910642  0.910051  0.893674  0.898745   \n",
       "forestreg train  0.352870  0.361841  0.353304  0.364521  0.354510  0.357409   \n",
       "forestreg val    0.844424  0.836910  0.854385  0.845759  0.823655  0.841027   \n",
       "linsvr train     0.907547  0.913225  0.904705  0.903970  0.908863  0.907662   \n",
       "linsvr val       0.908602  0.894419  0.923609  0.921272  0.904178  0.910416   \n",
       "neuralnet train  0.755310  0.748043  0.740307  0.751168  0.744720  0.747909   \n",
       "neuralnet val    0.824819  0.807315  0.827559  0.832562  0.826229  0.823697   \n",
       "\n",
       "                      std  \n",
       "baseline train   0.003609  \n",
       "baseline val     0.014327  \n",
       "linreg train     0.002937  \n",
       "linreg val       0.012113  \n",
       "forestreg train  0.005387  \n",
       "forestreg val    0.011523  \n",
       "linsvr train     0.003701  \n",
       "linsvr val       0.012145  \n",
       "neuralnet train  0.005774  \n",
       "neuralnet val    0.009612  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Standardization Only\")\n",
    "display(std_errors)\n",
    "\n",
    "print(\"\\n\\nPower Transform Only\")\n",
    "display(power_tr_model_errors)\n",
    "\n",
    "print(\"\\n\\nPower Transform and Drop 10 Least Important Features\")\n",
    "display(power_tr_drop_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_g3GFhVmBhfP"
   },
   "source": [
    "Notes about the models so far:\n",
    "- Power transformation to numerical columns results in less error except for the random forest model where the error increases slightly\n",
    "- Removing the least important features results in slightly more error except for the random forest model where error slightly decreases\n",
    "\n",
    "Best models so far:\n",
    "- Random Forest Regressor\n",
    "- Dense Neural Network"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOyG/Y5oofr0RsEDavKQcXE",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "predicting_imdb_ratings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
